{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42e244d0-062c-4949-8b8d-1ca9af0f8827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.cache/huggingface/hub/models--kharinaev--Llama-3.1-8B-Instruct-quip/snapshots/696a3fa489a5db8e27cbd42fc5a55da249ee7ee0/pytorch_model.bin'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "model_name = \"kharinaev/Llama-3.1-8B-Instruct-quip\"\n",
    "weights_loc = hf_hub_download(model_name, 'pytorch_model.bin')\n",
    "weights_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb1651c0-ef2b-4091-b347-8bc12ac8bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12K\n",
      "drwxr-xr-x 2 root root 4.0K Aug 27 15:20 .\n",
      "drwxr-xr-x 4 root root 4.0K Aug 26 19:26 ..\n",
      "lrwxrwxrwx 1 root root   52 Aug 27 15:20 .gitattributes -> ../../blobs/a6344aac8c09253b3b630fb776ae94478aa0275b\n",
      "lrwxrwxrwx 1 root root   52 Aug 26 19:26 config.json -> ../../blobs/f65b8d1f6fcb8d5c0bf9dcccd022b43cbc2e190e\n",
      "lrwxrwxrwx 1 root root   52 Aug 26 19:26 generation_config.json -> ../../blobs/cc7276afd599de091142c6ed3005faf8a74aa257\n",
      "lrwxrwxrwx 1 root root   76 Aug 26 20:25 pytorch_model.bin -> ../../blobs/988372232fb19988fbfb6873f11f14cb974d289fb0d4aa8ecfe6f5106304fb37\n",
      "lrwxrwxrwx 1 root root   52 Aug 27 15:20 quantization_config.json -> ../../blobs/a5f43489371e0d69f188522bd04f82477342f6c8\n",
      "lrwxrwxrwx 1 root root   52 Aug 26 19:26 special_tokens_map.json -> ../../blobs/02ee80b6196926a5ad790a004d9efd6ab1ba6542\n",
      "lrwxrwxrwx 1 root root   52 Aug 26 19:26 tokenizer.json -> ../../blobs/5cc5f00a5b203e90a27a3bd60d1ec393b07971e8\n",
      "lrwxrwxrwx 1 root root   52 Aug 26 19:26 tokenizer_config.json -> ../../blobs/db88166e2bc4c799fd5d1ae643b75e84d03ee70e\n"
     ]
    }
   ],
   "source": [
    "!ls -lha /root/.cache/huggingface/hub/models--kharinaev--Llama-3.1-8B-Instruct-quip/snapshots/696a3fa489a5db8e27cbd42fc5a55da249ee7ee0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e7ca4eb-8c04-4ef5-a604-93143dd95a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae289718-2918-489b-80db-f4e4636324da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, Union\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "from vllm.model_executor.models.llama import LlamaForCausalLM\n",
    "from vllm.attention import Attention, AttentionMetadata\n",
    "from vllm.sequence import IntermediateTensors, SamplerOutput\n",
    "\n",
    "from accelerate import (\n",
    "    Accelerator,\n",
    "    cpu_offload_with_hook,\n",
    "    init_empty_weights,\n",
    "    load_checkpoint_and_dispatch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a03895-b690-43f0-ba8e-017669a4ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_quant_layers(module, name=''):\n",
    "    for child_name, child in module.named_children():\n",
    "        full_name = f\"{name}.{child_name}\" if name else child_name\n",
    "        \n",
    "        if isinstance(child, torch.nn.Linear):\n",
    "            setattr(module, child_name, QuantLinear(\n",
    "                in_features=child.in_features,\n",
    "                out_features=child.out_features,\n",
    "                bias=child.bias is not None,\n",
    "                codebook=E8P12RVQ4B_codebook()\n",
    "            ))\n",
    "            print(f\"Replaced {full_name} with QuantLinear\")\n",
    "        else:\n",
    "            replace_with_quant_layers(child, full_name)\n",
    "\n",
    "def quantize_model(model):\n",
    "    # Замена линейных слоев на квантизованные\n",
    "    replace_with_quant_layers(model)\n",
    "    \n",
    "    # Замена LlamaAttention на LlamaSdpaAttention\n",
    "    for layer in model.model.layers:\n",
    "        old_attn = layer.self_attn\n",
    "        layer.self_attn = LlamaSdpaAttention(\n",
    "            hidden_size=old_attn.hidden_size,\n",
    "            num_heads=old_attn.num_heads\n",
    "        )\n",
    "        print(\"Replaced LlamaAttention with LlamaSdpaAttention\")\n",
    "    \n",
    "    # Замена VocabParallelEmbedding на обычный Embedding\n",
    "    model.model.embed_tokens = torch.nn.Embedding(\n",
    "        num_embeddings=model.model.embed_tokens.num_embeddings,\n",
    "        embedding_dim=model.model.embed_tokens.embedding_dim\n",
    "    )\n",
    "    print(\"Replaced VocabParallelEmbedding with Embedding\")\n",
    "    \n",
    "    # Замена ParallelLMHead на обычный Linear\n",
    "    model.lm_head = torch.nn.Linear(\n",
    "        in_features=model.lm_head.weight.shape[1],\n",
    "        out_features=model.lm_head.weight.shape[0],\n",
    "        bias=False\n",
    "    )\n",
    "    print(\"Replaced ParallelLMHead with Linear\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Загрузка неквантизованной модели\n",
    "model = LlamaForCausalLM.from_pretrained(\"path_to_your_model\")\n",
    "\n",
    "# Квантизация модели\n",
    "quantized_model = quantize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea8b0b-acbc-451a-b5ef-e6c4c04f1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from quip import QUIP\n",
    "from qlinear import QuantLinear\n",
    "# from codebook import codebook_id\n",
    "from codebook.e8p12_rvq4 import E8P12RVQ4B_codebook\n",
    "from quantizer import QuipQuantizer\n",
    "\n",
    "quip_quant_config = {\n",
    "    \"quant_method\": \"QUiP\",\n",
    "    \"rescale_WH\": False,\n",
    "    \"use_rand\": True,\n",
    "    \"codebook\": \"E8P12RVQ4B\",\n",
    "    \"codesz\": 8,\n",
    "    \"idx_dtype\": \"torch.int32\",\n",
    "    \"merge_suv\": False,\n",
    "    \"per_channel\": False,\n",
    "    \"opt_resid_scale\": -1,\n",
    "    \"modules_to_not_convert\": None,\n",
    "    \"inference\": True,\n",
    "    \"ft_epochs\": 0\n",
    "}\n",
    "\n",
    "class QuipLlamaForCausalLM(LlamaForCausalLM):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(config, **kwargs)\n",
    "        # self.codebook = codebook_id[config.quantization_config[\"codebook\"]](inference=True)\n",
    "        self.codebook = E8P12RVQ4B_codebook(inference=True)\n",
    "        self._replace_with_quant_layers()\n",
    "        self.config = config\n",
    "\n",
    "    def _replace_with_quant_layers(self):\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                in_features = module.in_features\n",
    "                out_features = module.out_features\n",
    "                bias = module.bias is not None\n",
    "                new_module = QuantLinear(\n",
    "                    in_features,\n",
    "                    out_features,\n",
    "                    self.codebook,\n",
    "                    bias=bias,\n",
    "                    use_rand=quip_quant_config[\"use_rand\"],\n",
    "                    per_channel=quip_quant_config[\"per_channel\"]\n",
    "                )\n",
    "                parent_name, child_name = name.rsplit('.', 1)\n",
    "                parent = self.get_submodule(parent_name)\n",
    "                setattr(parent, child_name, new_module)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def load_weights(model: \"QuipLlamaForCausalLM\", checkpoint_path: str):\n",
    "    #     state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    #     for name, param in model.named_parameters():\n",
    "    #         if name in state_dict:\n",
    "    #             if isinstance(param, QuantLinear):\n",
    "    #                 param.pack(state_dict[name], state_dict[f\"{name}_quantizer\"])\n",
    "    #             else:\n",
    "    #                 param.data.copy_(state_dict[name])\n",
    "    #     model.tie_weights()\n",
    "    def load_weights(self, checkpoint_path: str):\n",
    "\n",
    "        with init_empty_weights(include_buffers=False):\n",
    "            model = AutoModelForCausalLM.from_config(\n",
    "                self.config,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16)\n",
    "        # model = self.model\n",
    "            \n",
    "        quantizer = QuipQuantizer.from_dict(quip_quant_config)\n",
    "        model = quantizer.convert_model(model)\n",
    "\n",
    "        load_checkpoint_and_dispatch(\n",
    "            model,\n",
    "            checkpoint=checkpoint_path,\n",
    "            device_map=\"auto\",\n",
    "            no_split_module_classes=quantizer.get_no_split_module_classes(model),\n",
    "            dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "        model.is_quantized = True\n",
    "        model.eval()\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        positions: torch.Tensor,\n",
    "        kv_caches: List[torch.Tensor],\n",
    "        attn_metadata: AttentionMetadata,\n",
    "        intermediate_tensors: Optional[IntermediateTensors] = None,\n",
    "    ) -> Union[torch.Tensor, IntermediateTensors]:\n",
    "        model_output = self.model(input_ids, positions, kv_caches,\n",
    "                                  attn_metadata, intermediate_tensors)\n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35214286-eecb-46b4-9462-d80ccee02a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import ModelRegistry, LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# from quip_llama import QuipLlamaForCausalLM\n",
    "ModelRegistry.register_model(\"QuipLlamaForCausalLM\", QuipLlamaForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8fc3c1e-ce51-44ef-8825-2f02b36cde32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd78898-1c34-43e0-8dad-58579cb88f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(10)):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b252fc-0ff1-4a0d-b8a7-e61c9dcf8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'kharinaev/Llama-3.1-8B-Instruct-quip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "641cb486-4532-468f-81b0-83996457312d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-26 20:39:51 arg_utils.py:839] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 08-26 20:39:51 config.py:911] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 08-26 20:39:51 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='kharinaev/Llama-3.1-8B-Instruct-quip', speculative_config=None, tokenizer='kharinaev/Llama-3.1-8B-Instruct-quip', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=kharinaev/Llama-3.1-8B-Instruct-quip, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-26 20:39:53 model_runner.py:879] Starting to load model kharinaev/Llama-3.1-8B-Instruct-quip...\n",
      "INFO 08-26 20:39:53 weight_utils.py:236] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not generator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/vllm/entrypoints/llm.py:175\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no need to pass vision-related arguments anymore.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    155\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    156\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    174\u001b[0m )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/vllm/engine/llm_engine.py:473\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    471\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/vllm/engine/llm_engine.py:270\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers, input_registry)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_registry \u001b[38;5;241m=\u001b[39m input_registry\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m input_registry\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    268\u001b[0m     model_config)\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservability_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservability_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/vllm/executor/executor_base.py:46\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, prompt_adapter_config, observability_config)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m prompt_adapter_config\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;241m=\u001b[39m observability_config\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/vllm/executor/gpu_executor.py:39\u001b[0m, in \u001b[0;36mGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39minit_device()\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/vllm/worker/worker.py:182\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/vllm/worker/model_runner.py:881\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    879\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to load model \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m CudaMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m--> 881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m    890\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    891\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/vllm/model_executor/model_loader/__init__.py:19\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config, load_config, device_config, parallel_config, scheduler_config, lora_config, cache_config)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;241m*\u001b[39m, model_config: ModelConfig, load_config: LoadConfig,\n\u001b[1;32m     14\u001b[0m               device_config: DeviceConfig, parallel_config: ParallelConfig,\n\u001b[1;32m     15\u001b[0m               scheduler_config: SchedulerConfig,\n\u001b[1;32m     16\u001b[0m               lora_config: Optional[LoRAConfig],\n\u001b[1;32m     17\u001b[0m               cache_config: CacheConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     18\u001b[0m     loader \u001b[38;5;241m=\u001b[39m get_model_loader(load_config)\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/vllm/model_executor/model_loader/loader.py:344\u001b[0m, in \u001b[0;36mDefaultModelLoader.load_model\u001b[0;34m(self, model_config, device_config, lora_config, parallel_config, scheduler_config, cache_config)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m target_device:\n\u001b[1;32m    341\u001b[0m     model \u001b[38;5;241m=\u001b[39m _initialize_model(model_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_config,\n\u001b[1;32m    342\u001b[0m                               lora_config, cache_config,\n\u001b[1;32m    343\u001b[0m                               scheduler_config)\n\u001b[0;32m--> 344\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_weights_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mfall_back_to_pt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfall_back_to_pt_during_load\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[1;32m    353\u001b[0m     quant_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 79\u001b[0m, in \u001b[0;36mQuipLlamaForCausalLM.load_weights\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m     76\u001b[0m quantizer \u001b[38;5;241m=\u001b[39m QuipQuantizer\u001b[38;5;241m.\u001b[39mfrom_dict(quip_quant_config)\n\u001b[1;32m     77\u001b[0m model \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mconvert_model(model)\n\u001b[0;32m---> 79\u001b[0m \u001b[43mload_checkpoint_and_dispatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_split_module_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_no_split_module_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m model\u001b[38;5;241m.\u001b[39mis_quantized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     88\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/accelerate/big_modeling.py:613\u001b[0m, in \u001b[0;36mload_checkpoint_and_dispatch\u001b[0;34m(model, checkpoint, device_map, max_memory, no_split_module_classes, offload_folder, offload_buffers, dtype, offload_state_dict, skip_keys, preload_module_classes, force_hooks, strict)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offload_state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    612\u001b[0m     offload_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 613\u001b[0m \u001b[43mload_checkpoint_in_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/accelerate/utils/modeling.py:1705\u001b[0m, in \u001b[0;36mload_checkpoint_in_model\u001b[0;34m(model, checkpoint, device_map, offload_folder, dtype, offload_state_dict, offload_buffers, keep_in_fp32_modules, offload_8bit_bnb, strict)\u001b[0m\n\u001b[1;32m   1703\u001b[0m checkpoint_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m index_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1706\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(checkpoint)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1707\u001b[0m         index_filename \u001b[38;5;241m=\u001b[39m checkpoint\n",
      "File \u001b[0;32m/usr/lib/python3.8/genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not generator"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=model_name, \n",
    "    trust_remote_code=True,\n",
    "    enforce_eager=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "235986da-3680-4bbc-b52d-a779c9c4cbe6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/usr/lib/python3.8/genericpath.py\u001b[0m(30)\u001b[0;36misfile\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     28 \u001b[0;31m    \u001b[0;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m    \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 30 \u001b[0;31m        \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     31 \u001b[0;31m    \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     32 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  globals()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__name__': 'genericpath', '__doc__': '\\nPath operations common to more than one OS\\nDo not use directly.  The OS specific modules import the appropriate\\nfunctions from this module themselves.\\n', '__package__': '', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7fcf46cb3f70>, '__spec__': ModuleSpec(name='genericpath', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7fcf46cb3f70>, origin='/usr/lib/python3.8/genericpath.py'), '__file__': '/usr/lib/python3.8/genericpath.py', '__cached__': '/usr/lib/python3.8/__pycache__/genericpath.cpython-38.pyc', '__builtins__': {'__name__': 'builtins', '__doc__': \"Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: None is the `nil' object; Ellipsis represents `...' in slices.\", '__package__': '', '__loader__': <class '_frozen_importlib.BuiltinImporter'>, '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>), '__build_class__': <built-in function __build_class__>, '__import__': <built-in function __import__>, 'abs': <built-in function abs>, 'all': <built-in function all>, 'any': <built-in function any>, 'ascii': <built-in function ascii>, 'bin': <built-in function bin>, 'breakpoint': <built-in function breakpoint>, 'callable': <built-in function callable>, 'chr': <built-in function chr>, 'compile': <built-in function compile>, 'delattr': <built-in function delattr>, 'dir': <built-in function dir>, 'divmod': <built-in function divmod>, 'eval': <built-in function eval>, 'exec': <built-in function exec>, 'format': <built-in function format>, 'getattr': <built-in function getattr>, 'globals': <built-in function globals>, 'hasattr': <built-in function hasattr>, 'hash': <built-in function hash>, 'hex': <built-in function hex>, 'id': <built-in function id>, 'input': <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7fcf4012d2b0>>, 'isinstance': <built-in function isinstance>, 'issubclass': <built-in function issubclass>, 'iter': <built-in function iter>, 'len': <built-in function len>, 'locals': <built-in function locals>, 'max': <built-in function max>, 'min': <built-in function min>, 'next': <built-in function next>, 'oct': <built-in function oct>, 'ord': <built-in function ord>, 'pow': <built-in function pow>, 'print': <built-in function print>, 'repr': <built-in function repr>, 'round': <built-in function round>, 'setattr': <built-in function setattr>, 'sorted': <built-in function sorted>, 'sum': <built-in function sum>, 'vars': <built-in function vars>, 'None': None, 'Ellipsis': Ellipsis, 'NotImplemented': NotImplemented, 'False': False, 'True': True, 'bool': <class 'bool'>, 'memoryview': <class 'memoryview'>, 'bytearray': <class 'bytearray'>, 'bytes': <class 'bytes'>, 'classmethod': <class 'classmethod'>, 'complex': <class 'complex'>, 'dict': <class 'dict'>, 'enumerate': <class 'enumerate'>, 'filter': <class 'filter'>, 'float': <class 'float'>, 'frozenset': <class 'frozenset'>, 'property': <class 'property'>, 'int': <class 'int'>, 'list': <class 'list'>, 'map': <class 'map'>, 'object': <class 'object'>, 'range': <class 'range'>, 'reversed': <class 'reversed'>, 'set': <class 'set'>, 'slice': <class 'slice'>, 'staticmethod': <class 'staticmethod'>, 'str': <class 'str'>, 'super': <class 'super'>, 'tuple': <class 'tuple'>, 'type': <class 'type'>, 'zip': <class 'zip'>, '__debug__': True, 'BaseException': <class 'BaseException'>, 'Exception': <class 'Exception'>, 'TypeError': <class 'TypeError'>, 'StopAsyncIteration': <class 'StopAsyncIteration'>, 'StopIteration': <class 'StopIteration'>, 'GeneratorExit': <class 'GeneratorExit'>, 'SystemExit': <class 'SystemExit'>, 'KeyboardInterrupt': <class 'KeyboardInterrupt'>, 'ImportError': <class 'ImportError'>, 'ModuleNotFoundError': <class 'ModuleNotFoundError'>, 'OSError': <class 'OSError'>, 'EnvironmentError': <class 'OSError'>, 'IOError': <class 'OSError'>, 'EOFError': <class 'EOFError'>, 'RuntimeError': <class 'RuntimeError'>, 'RecursionError': <class 'RecursionError'>, 'NotImplementedError': <class 'NotImplementedError'>, 'NameError': <class 'NameError'>, 'UnboundLocalError': <class 'UnboundLocalError'>, 'AttributeError': <class 'AttributeError'>, 'SyntaxError': <class 'SyntaxError'>, 'IndentationError': <class 'IndentationError'>, 'TabError': <class 'TabError'>, 'LookupError': <class 'LookupError'>, 'IndexError': <class 'IndexError'>, 'KeyError': <class 'KeyError'>, 'ValueError': <class 'ValueError'>, 'UnicodeError': <class 'UnicodeError'>, 'UnicodeEncodeError': <class 'UnicodeEncodeError'>, 'UnicodeDecodeError': <class 'UnicodeDecodeError'>, 'UnicodeTranslateError': <class 'UnicodeTranslateError'>, 'AssertionError': <class 'AssertionError'>, 'ArithmeticError': <class 'ArithmeticError'>, 'FloatingPointError': <class 'FloatingPointError'>, 'OverflowError': <class 'OverflowError'>, 'ZeroDivisionError': <class 'ZeroDivisionError'>, 'SystemError': <class 'SystemError'>, 'ReferenceError': <class 'ReferenceError'>, 'MemoryError': <class 'MemoryError'>, 'BufferError': <class 'BufferError'>, 'Warning': <class 'Warning'>, 'UserWarning': <class 'UserWarning'>, 'DeprecationWarning': <class 'DeprecationWarning'>, 'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>, 'SyntaxWarning': <class 'SyntaxWarning'>, 'RuntimeWarning': <class 'RuntimeWarning'>, 'FutureWarning': <class 'FutureWarning'>, 'ImportWarning': <class 'ImportWarning'>, 'UnicodeWarning': <class 'UnicodeWarning'>, 'BytesWarning': <class 'BytesWarning'>, 'ResourceWarning': <class 'ResourceWarning'>, 'ConnectionError': <class 'ConnectionError'>, 'BlockingIOError': <class 'BlockingIOError'>, 'BrokenPipeError': <class 'BrokenPipeError'>, 'ChildProcessError': <class 'ChildProcessError'>, 'ConnectionAbortedError': <class 'ConnectionAbortedError'>, 'ConnectionRefusedError': <class 'ConnectionRefusedError'>, 'ConnectionResetError': <class 'ConnectionResetError'>, 'FileExistsError': <class 'FileExistsError'>, 'FileNotFoundError': <class 'FileNotFoundError'>, 'IsADirectoryError': <class 'IsADirectoryError'>, 'NotADirectoryError': <class 'NotADirectoryError'>, 'InterruptedError': <class 'InterruptedError'>, 'PermissionError': <class 'PermissionError'>, 'ProcessLookupError': <class 'ProcessLookupError'>, 'TimeoutError': <class 'TimeoutError'>, 'open': <built-in function open>, 'copyright': Copyright (c) 2001-2021 Python Software Foundation.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 2000 BeOpen.com.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
      "All Rights Reserved., 'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
      "    for supporting Python development.  See www.python.org for more information., 'license': Type license() to see the full license text, 'help': Type help() for interactive help, or help(object) for help about object., 'execfile': <function execfile at 0x7fcf40093820>, 'runfile': <function runfile at 0x7fcf2b6bab80>, '__IPYTHON__': True, 'display': <function display at 0x7fcf44fee280>, '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1011__': <capsule object NULL at 0x7fcf284713f0>, '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1014__': <capsule object NULL at 0x7fcdc73820c0>, '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1017__': <capsule object NULL at 0x7fcda7b7c810>, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fcf2b46c160>>}, 'os': <module 'os' from '/usr/lib/python3.8/os.py'>, 'stat': <module 'stat' from '/usr/lib/python3.8/stat.py'>, '__all__': ['commonprefix', 'exists', 'getatime', 'getctime', 'getmtime', 'getsize', 'isdir', 'isfile', 'samefile', 'sameopenfile', 'samestat'], 'exists': <function exists at 0x7fcf46cbedc0>, 'isfile': <function isfile at 0x7fcf46cbee50>, 'isdir': <function isdir at 0x7fcf46cbeee0>, 'getsize': <function getsize at 0x7fcf46cbef70>, 'getmtime': <function getmtime at 0x7fcf46cc5040>, 'getatime': <function getatime at 0x7fcf46cc50d0>, 'getctime': <function getctime at 0x7fcf46cc5160>, 'commonprefix': <function commonprefix at 0x7fcf46cc51f0>, 'samestat': <function samestat at 0x7fcf46cc5280>, 'samefile': <function samefile at 0x7fcf46cc5310>, 'sameopenfile': <function sameopenfile at 0x7fcf46cc53a0>, '_splitext': <function _splitext at 0x7fcf46cc5430>, '_check_arg_types': <function _check_arg_types at 0x7fcf46cc54c0>, 'path': <generator object pt_weights_iterator at 0x7fc7f022deb0>}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  import sys\n",
      "ipdb>  sys.modules['__main__'].weight_generator = path\n",
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39596cce-edd3-4615-9d09-fe58cef8a2ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63651a4a-03a8-4d0a-85bb-697021ffe9e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  'import os\\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"',\n",
       "  'import torch\\nfrom vllm.model_executor.models.llama import LlamaForCausalLM\\nfrom transformers import AutoTokenizer, Auto\\nConfig, AutoModelForCausalLM\\n# from quip import QUIP\\nfrom qlinear import QuantLinear\\n# from codebook import codebook_id\\nfrom codebook.e8p12_rvq4 import E8P12RVQ4B_codebook\\nfrom accelerate import (\\n    Accelerator,\\n    cpu_offload_with_hook,\\n    init_empty_weights,\\n    load_checkpoint_and_dispatch\\n)\\nfrom quantizer import QuipQuantizer\\n\\nquip_quant_config = {\\n    \"quant_method\": \"QUiP\",\\n    \"rescale_WH\": False,\\n    \"use_rand\": True,\\n    \"codebook\": \"E8P12RVQ4B\",\\n    \"codesz\": 8,\\n    \"idx_dtype\": \"torch.int32\",\\n    \"merge_suv\": False,\\n    \"per_channel\": False,\\n    \"opt_resid_scale\": -1,\\n    \"modules_to_not_convert\": None,\\n    \"inference\": True,\\n    \"ft_epochs\": 0\\n}\\n\\nclass QuipLlamaForCausalLM(LlamaForCausalLM):\\n    def __init__(self, config, **kwargs):\\n        super().__init__(config, **kwargs)\\n        # self.codebook = codebook_id[config.quantization_config[\"codebook\"]](inference=True)\\n        self.codebook = E8P12RVQ4B_codebook(inference=True)\\n        self._replace_with_quant_layers()\\n        self.config = config\\n\\n    def _replace_with_quant_layers(self):\\n        for name, module in self.named_modules():\\n            if isinstance(module, torch.nn.Linear):\\n                in_features = module.in_features\\n                out_features = module.out_features\\n                bias = module.bias is not None\\n                new_module = QuantLinear(\\n                    in_features,\\n                    out_features,\\n                    self.codebook,\\n                    bias=bias,\\n                    use_rand=quip_quant_config[\"use_rand\"],\\n                    per_channel=quip_quant_config[\"per_channel\"]\\n                )\\n                parent_name, child_name = name.rsplit(\\'.\\', 1)\\n                parent = self.get_submodule(parent_name)\\n                setattr(parent, child_name, new_module)\\n\\n    # @staticmethod\\n    # def load_weights(model: \"QuipLlamaForCausalLM\", checkpoint_path: str):\\n    #     state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\\n    #     for name, param in model.named_parameters():\\n    #         if name in state_dict:\\n    #             if isinstance(param, QuantLinear):\\n    #                 param.pack(state_dict[name], state_dict[f\"{name}_quantizer\"])\\n    #             else:\\n    #                 param.data.copy_(state_dict[name])\\n    #     model.tie_weights()\\n    def load_weights(self, checkpoint_path: str):\\n\\n        with init_empty_weights(include_buffers=False):\\n            model = AutoModelForCausalLM.from_config(\\n                self.config,\\n                trust_remote_code=True,\\n                torch_dtype=torch.float16)\\n        # model = self.model\\n            \\n        quantizer = QuipQuantizer.from_dict(quip_quant_config)\\n        model = quantizer.convert_model(model)\\n\\n        load_checkpoint_and_dispatch(\\n            model,\\n            checkpoint=checkpoint_path,\\n            device_map=\"auto\",\\n            no_split_module_classes=quantizer.get_no_split_module_classes(model),\\n            dtype=torch.float16,\\n        )\\n\\n        model.is_quantized = True\\n        model.eval()\\n        self.model = model\\n        return model\\n\\n    def forward(self, **kwargs):\\n        return self.model(**kwargs)',\n",
       "  'import torch\\nfrom vllm.model_executor.models.llama import LlamaForCausalLM\\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\\n# from quip import QUIP\\nfrom qlinear import QuantLinear\\n# from codebook import codebook_id\\nfrom codebook.e8p12_rvq4 import E8P12RVQ4B_codebook\\nfrom accelerate import (\\n    Accelerator,\\n    cpu_offload_with_hook,\\n    init_empty_weights,\\n    load_checkpoint_and_dispatch\\n)\\nfrom quantizer import QuipQuantizer\\n\\nquip_quant_config = {\\n    \"quant_method\": \"QUiP\",\\n    \"rescale_WH\": False,\\n    \"use_rand\": True,\\n    \"codebook\": \"E8P12RVQ4B\",\\n    \"codesz\": 8,\\n    \"idx_dtype\": \"torch.int32\",\\n    \"merge_suv\": False,\\n    \"per_channel\": False,\\n    \"opt_resid_scale\": -1,\\n    \"modules_to_not_convert\": None,\\n    \"inference\": True,\\n    \"ft_epochs\": 0\\n}\\n\\nclass QuipLlamaForCausalLM(LlamaForCausalLM):\\n    def __init__(self, config, **kwargs):\\n        super().__init__(config, **kwargs)\\n        # self.codebook = codebook_id[config.quantization_config[\"codebook\"]](inference=True)\\n        self.codebook = E8P12RVQ4B_codebook(inference=True)\\n        self._replace_with_quant_layers()\\n        self.config = config\\n\\n    def _replace_with_quant_layers(self):\\n        for name, module in self.named_modules():\\n            if isinstance(module, torch.nn.Linear):\\n                in_features = module.in_features\\n                out_features = module.out_features\\n                bias = module.bias is not None\\n                new_module = QuantLinear(\\n                    in_features,\\n                    out_features,\\n                    self.codebook,\\n                    bias=bias,\\n                    use_rand=quip_quant_config[\"use_rand\"],\\n                    per_channel=quip_quant_config[\"per_channel\"]\\n                )\\n                parent_name, child_name = name.rsplit(\\'.\\', 1)\\n                parent = self.get_submodule(parent_name)\\n                setattr(parent, child_name, new_module)\\n\\n    # @staticmethod\\n    # def load_weights(model: \"QuipLlamaForCausalLM\", checkpoint_path: str):\\n    #     state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\\n    #     for name, param in model.named_parameters():\\n    #         if name in state_dict:\\n    #             if isinstance(param, QuantLinear):\\n    #                 param.pack(state_dict[name], state_dict[f\"{name}_quantizer\"])\\n    #             else:\\n    #                 param.data.copy_(state_dict[name])\\n    #     model.tie_weights()\\n    def load_weights(self, checkpoint_path: str):\\n\\n        with init_empty_weights(include_buffers=False):\\n            model = AutoModelForCausalLM.from_config(\\n                self.config,\\n                trust_remote_code=True,\\n                torch_dtype=torch.float16)\\n        # model = self.model\\n            \\n        quantizer = QuipQuantizer.from_dict(quip_quant_config)\\n        model = quantizer.convert_model(model)\\n\\n        load_checkpoint_and_dispatch(\\n            model,\\n            checkpoint=checkpoint_path,\\n            device_map=\"auto\",\\n            no_split_module_classes=quantizer.get_no_split_module_classes(model),\\n            dtype=torch.float16,\\n        )\\n\\n        model.is_quantized = True\\n        model.eval()\\n        self.model = model\\n        return model\\n\\n    def forward(self, **kwargs):\\n        return self.model(**kwargs)',\n",
       "  'from vllm import ModelRegistry, LLM, SamplingParams\\nfrom transformers import AutoTokenizer\\n\\n# from quip_llama import QuipLlamaForCausalLM\\nModelRegistry.register_model(\"QuipLlamaForCausalLM\", QuipLlamaForCausalLM)',\n",
       "  'from tqdm import tqdm',\n",
       "  '# for i in tqdm(range(10)):\\n#     pass',\n",
       "  \"model_name = 'kharinaev/Llama-3.1-8B-Instruct-quip'\",\n",
       "  'llm = LLM(\\n    model=model_name, \\n    trust_remote_code=True,\\n    enforce_eager=True,\\n)',\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  'llm',\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  'llm = LLM(\\n    model=model_name, \\n    trust_remote_code=True,\\n    enforce_eager=True,\\n)',\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  'globals()',\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  'llm = LLM(\\n    model=model_name, \\n    trust_remote_code=True,\\n    enforce_eager=True,\\n)',\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  'globals()'],\n",
       " '_oh': {15: {...}},\n",
       " '_dh': [PosixPath('/root/llm_quant_safety/quantization/QuIP-for-all')],\n",
       " 'In': ['',\n",
       "  'import os\\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"',\n",
       "  'import torch\\nfrom vllm.model_executor.models.llama import LlamaForCausalLM\\nfrom transformers import AutoTokenizer, Auto\\nConfig, AutoModelForCausalLM\\n# from quip import QUIP\\nfrom qlinear import QuantLinear\\n# from codebook import codebook_id\\nfrom codebook.e8p12_rvq4 import E8P12RVQ4B_codebook\\nfrom accelerate import (\\n    Accelerator,\\n    cpu_offload_with_hook,\\n    init_empty_weights,\\n    load_checkpoint_and_dispatch\\n)\\nfrom quantizer import QuipQuantizer\\n\\nquip_quant_config = {\\n    \"quant_method\": \"QUiP\",\\n    \"rescale_WH\": False,\\n    \"use_rand\": True,\\n    \"codebook\": \"E8P12RVQ4B\",\\n    \"codesz\": 8,\\n    \"idx_dtype\": \"torch.int32\",\\n    \"merge_suv\": False,\\n    \"per_channel\": False,\\n    \"opt_resid_scale\": -1,\\n    \"modules_to_not_convert\": None,\\n    \"inference\": True,\\n    \"ft_epochs\": 0\\n}\\n\\nclass QuipLlamaForCausalLM(LlamaForCausalLM):\\n    def __init__(self, config, **kwargs):\\n        super().__init__(config, **kwargs)\\n        # self.codebook = codebook_id[config.quantization_config[\"codebook\"]](inference=True)\\n        self.codebook = E8P12RVQ4B_codebook(inference=True)\\n        self._replace_with_quant_layers()\\n        self.config = config\\n\\n    def _replace_with_quant_layers(self):\\n        for name, module in self.named_modules():\\n            if isinstance(module, torch.nn.Linear):\\n                in_features = module.in_features\\n                out_features = module.out_features\\n                bias = module.bias is not None\\n                new_module = QuantLinear(\\n                    in_features,\\n                    out_features,\\n                    self.codebook,\\n                    bias=bias,\\n                    use_rand=quip_quant_config[\"use_rand\"],\\n                    per_channel=quip_quant_config[\"per_channel\"]\\n                )\\n                parent_name, child_name = name.rsplit(\\'.\\', 1)\\n                parent = self.get_submodule(parent_name)\\n                setattr(parent, child_name, new_module)\\n\\n    # @staticmethod\\n    # def load_weights(model: \"QuipLlamaForCausalLM\", checkpoint_path: str):\\n    #     state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\\n    #     for name, param in model.named_parameters():\\n    #         if name in state_dict:\\n    #             if isinstance(param, QuantLinear):\\n    #                 param.pack(state_dict[name], state_dict[f\"{name}_quantizer\"])\\n    #             else:\\n    #                 param.data.copy_(state_dict[name])\\n    #     model.tie_weights()\\n    def load_weights(self, checkpoint_path: str):\\n\\n        with init_empty_weights(include_buffers=False):\\n            model = AutoModelForCausalLM.from_config(\\n                self.config,\\n                trust_remote_code=True,\\n                torch_dtype=torch.float16)\\n        # model = self.model\\n            \\n        quantizer = QuipQuantizer.from_dict(quip_quant_config)\\n        model = quantizer.convert_model(model)\\n\\n        load_checkpoint_and_dispatch(\\n            model,\\n            checkpoint=checkpoint_path,\\n            device_map=\"auto\",\\n            no_split_module_classes=quantizer.get_no_split_module_classes(model),\\n            dtype=torch.float16,\\n        )\\n\\n        model.is_quantized = True\\n        model.eval()\\n        self.model = model\\n        return model\\n\\n    def forward(self, **kwargs):\\n        return self.model(**kwargs)',\n",
       "  'import torch\\nfrom vllm.model_executor.models.llama import LlamaForCausalLM\\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\\n# from quip import QUIP\\nfrom qlinear import QuantLinear\\n# from codebook import codebook_id\\nfrom codebook.e8p12_rvq4 import E8P12RVQ4B_codebook\\nfrom accelerate import (\\n    Accelerator,\\n    cpu_offload_with_hook,\\n    init_empty_weights,\\n    load_checkpoint_and_dispatch\\n)\\nfrom quantizer import QuipQuantizer\\n\\nquip_quant_config = {\\n    \"quant_method\": \"QUiP\",\\n    \"rescale_WH\": False,\\n    \"use_rand\": True,\\n    \"codebook\": \"E8P12RVQ4B\",\\n    \"codesz\": 8,\\n    \"idx_dtype\": \"torch.int32\",\\n    \"merge_suv\": False,\\n    \"per_channel\": False,\\n    \"opt_resid_scale\": -1,\\n    \"modules_to_not_convert\": None,\\n    \"inference\": True,\\n    \"ft_epochs\": 0\\n}\\n\\nclass QuipLlamaForCausalLM(LlamaForCausalLM):\\n    def __init__(self, config, **kwargs):\\n        super().__init__(config, **kwargs)\\n        # self.codebook = codebook_id[config.quantization_config[\"codebook\"]](inference=True)\\n        self.codebook = E8P12RVQ4B_codebook(inference=True)\\n        self._replace_with_quant_layers()\\n        self.config = config\\n\\n    def _replace_with_quant_layers(self):\\n        for name, module in self.named_modules():\\n            if isinstance(module, torch.nn.Linear):\\n                in_features = module.in_features\\n                out_features = module.out_features\\n                bias = module.bias is not None\\n                new_module = QuantLinear(\\n                    in_features,\\n                    out_features,\\n                    self.codebook,\\n                    bias=bias,\\n                    use_rand=quip_quant_config[\"use_rand\"],\\n                    per_channel=quip_quant_config[\"per_channel\"]\\n                )\\n                parent_name, child_name = name.rsplit(\\'.\\', 1)\\n                parent = self.get_submodule(parent_name)\\n                setattr(parent, child_name, new_module)\\n\\n    # @staticmethod\\n    # def load_weights(model: \"QuipLlamaForCausalLM\", checkpoint_path: str):\\n    #     state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\\n    #     for name, param in model.named_parameters():\\n    #         if name in state_dict:\\n    #             if isinstance(param, QuantLinear):\\n    #                 param.pack(state_dict[name], state_dict[f\"{name}_quantizer\"])\\n    #             else:\\n    #                 param.data.copy_(state_dict[name])\\n    #     model.tie_weights()\\n    def load_weights(self, checkpoint_path: str):\\n\\n        with init_empty_weights(include_buffers=False):\\n            model = AutoModelForCausalLM.from_config(\\n                self.config,\\n                trust_remote_code=True,\\n                torch_dtype=torch.float16)\\n        # model = self.model\\n            \\n        quantizer = QuipQuantizer.from_dict(quip_quant_config)\\n        model = quantizer.convert_model(model)\\n\\n        load_checkpoint_and_dispatch(\\n            model,\\n            checkpoint=checkpoint_path,\\n            device_map=\"auto\",\\n            no_split_module_classes=quantizer.get_no_split_module_classes(model),\\n            dtype=torch.float16,\\n        )\\n\\n        model.is_quantized = True\\n        model.eval()\\n        self.model = model\\n        return model\\n\\n    def forward(self, **kwargs):\\n        return self.model(**kwargs)',\n",
       "  'from vllm import ModelRegistry, LLM, SamplingParams\\nfrom transformers import AutoTokenizer\\n\\n# from quip_llama import QuipLlamaForCausalLM\\nModelRegistry.register_model(\"QuipLlamaForCausalLM\", QuipLlamaForCausalLM)',\n",
       "  'from tqdm import tqdm',\n",
       "  '# for i in tqdm(range(10)):\\n#     pass',\n",
       "  \"model_name = 'kharinaev/Llama-3.1-8B-Instruct-quip'\",\n",
       "  'llm = LLM(\\n    model=model_name, \\n    trust_remote_code=True,\\n    enforce_eager=True,\\n)',\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  'llm',\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  'llm = LLM(\\n    model=model_name, \\n    trust_remote_code=True,\\n    enforce_eager=True,\\n)',\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  'globals()',\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  'llm = LLM(\\n    model=model_name, \\n    trust_remote_code=True,\\n    enforce_eager=True,\\n)',\n",
       "  \"get_ipython().run_line_magic('debug', '')\",\n",
       "  'globals()'],\n",
       " 'Out': {15: {...}},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fcf2b46c160>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7fcf2b46cfa0>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7fcf2b46cfa0>,\n",
       " 'open': <function io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)>,\n",
       " '_': {...},\n",
       " '__': '',\n",
       " '___': '',\n",
       " '__session__': '/root/llm_quant_safety/quantization/QuIP-for-all/adapt_quip_vllm.ipynb',\n",
       " '_i': '%debug',\n",
       " '_ii': 'llm = LLM(\\n    model=model_name, \\n    trust_remote_code=True,\\n    enforce_eager=True,\\n)',\n",
       " '_iii': '%debug',\n",
       " '_i1': 'import os\\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"',\n",
       " 'os': <module 'os' from '/usr/lib/python3.8/os.py'>,\n",
       " '_i2': 'import torch\\nfrom vllm.model_executor.models.llama import LlamaForCausalLM\\nfrom transformers import AutoTokenizer, Auto\\nConfig, AutoModelForCausalLM\\n# from quip import QUIP\\nfrom qlinear import QuantLinear\\n# from codebook import codebook_id\\nfrom codebook.e8p12_rvq4 import E8P12RVQ4B_codebook\\nfrom accelerate import (\\n    Accelerator,\\n    cpu_offload_with_hook,\\n    init_empty_weights,\\n    load_checkpoint_and_dispatch\\n)\\nfrom quantizer import QuipQuantizer\\n\\nquip_quant_config = {\\n    \"quant_method\": \"QUiP\",\\n    \"rescale_WH\": False,\\n    \"use_rand\": True,\\n    \"codebook\": \"E8P12RVQ4B\",\\n    \"codesz\": 8,\\n    \"idx_dtype\": \"torch.int32\",\\n    \"merge_suv\": False,\\n    \"per_channel\": False,\\n    \"opt_resid_scale\": -1,\\n    \"modules_to_not_convert\": None,\\n    \"inference\": True,\\n    \"ft_epochs\": 0\\n}\\n\\nclass QuipLlamaForCausalLM(LlamaForCausalLM):\\n    def __init__(self, config, **kwargs):\\n        super().__init__(config, **kwargs)\\n        # self.codebook = codebook_id[config.quantization_config[\"codebook\"]](inference=True)\\n        self.codebook = E8P12RVQ4B_codebook(inference=True)\\n        self._replace_with_quant_layers()\\n        self.config = config\\n\\n    def _replace_with_quant_layers(self):\\n        for name, module in self.named_modules():\\n            if isinstance(module, torch.nn.Linear):\\n                in_features = module.in_features\\n                out_features = module.out_features\\n                bias = module.bias is not None\\n                new_module = QuantLinear(\\n                    in_features,\\n                    out_features,\\n                    self.codebook,\\n                    bias=bias,\\n                    use_rand=quip_quant_config[\"use_rand\"],\\n                    per_channel=quip_quant_config[\"per_channel\"]\\n                )\\n                parent_name, child_name = name.rsplit(\\'.\\', 1)\\n                parent = self.get_submodule(parent_name)\\n                setattr(parent, child_name, new_module)\\n\\n    # @staticmethod\\n    # def load_weights(model: \"QuipLlamaForCausalLM\", checkpoint_path: str):\\n    #     state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\\n    #     for name, param in model.named_parameters():\\n    #         if name in state_dict:\\n    #             if isinstance(param, QuantLinear):\\n    #                 param.pack(state_dict[name], state_dict[f\"{name}_quantizer\"])\\n    #             else:\\n    #                 param.data.copy_(state_dict[name])\\n    #     model.tie_weights()\\n    def load_weights(self, checkpoint_path: str):\\n\\n        with init_empty_weights(include_buffers=False):\\n            model = AutoModelForCausalLM.from_config(\\n                self.config,\\n                trust_remote_code=True,\\n                torch_dtype=torch.float16)\\n        # model = self.model\\n            \\n        quantizer = QuipQuantizer.from_dict(quip_quant_config)\\n        model = quantizer.convert_model(model)\\n\\n        load_checkpoint_and_dispatch(\\n            model,\\n            checkpoint=checkpoint_path,\\n            device_map=\"auto\",\\n            no_split_module_classes=quantizer.get_no_split_module_classes(model),\\n            dtype=torch.float16,\\n        )\\n\\n        model.is_quantized = True\\n        model.eval()\\n        self.model = model\\n        return model\\n\\n    def forward(self, **kwargs):\\n        return self.model(**kwargs)',\n",
       " 'torch': <module 'torch' from '/usr/local/lib/python3.8/dist-packages/torch/__init__.py'>,\n",
       " 'LlamaForCausalLM': vllm.model_executor.models.llama.LlamaForCausalLM,\n",
       " 'AutoTokenizer': transformers.models.auto.tokenization_auto.AutoTokenizer,\n",
       " '_i3': 'import torch\\nfrom vllm.model_executor.models.llama import LlamaForCausalLM\\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\\n# from quip import QUIP\\nfrom qlinear import QuantLinear\\n# from codebook import codebook_id\\nfrom codebook.e8p12_rvq4 import E8P12RVQ4B_codebook\\nfrom accelerate import (\\n    Accelerator,\\n    cpu_offload_with_hook,\\n    init_empty_weights,\\n    load_checkpoint_and_dispatch\\n)\\nfrom quantizer import QuipQuantizer\\n\\nquip_quant_config = {\\n    \"quant_method\": \"QUiP\",\\n    \"rescale_WH\": False,\\n    \"use_rand\": True,\\n    \"codebook\": \"E8P12RVQ4B\",\\n    \"codesz\": 8,\\n    \"idx_dtype\": \"torch.int32\",\\n    \"merge_suv\": False,\\n    \"per_channel\": False,\\n    \"opt_resid_scale\": -1,\\n    \"modules_to_not_convert\": None,\\n    \"inference\": True,\\n    \"ft_epochs\": 0\\n}\\n\\nclass QuipLlamaForCausalLM(LlamaForCausalLM):\\n    def __init__(self, config, **kwargs):\\n        super().__init__(config, **kwargs)\\n        # self.codebook = codebook_id[config.quantization_config[\"codebook\"]](inference=True)\\n        self.codebook = E8P12RVQ4B_codebook(inference=True)\\n        self._replace_with_quant_layers()\\n        self.config = config\\n\\n    def _replace_with_quant_layers(self):\\n        for name, module in self.named_modules():\\n            if isinstance(module, torch.nn.Linear):\\n                in_features = module.in_features\\n                out_features = module.out_features\\n                bias = module.bias is not None\\n                new_module = QuantLinear(\\n                    in_features,\\n                    out_features,\\n                    self.codebook,\\n                    bias=bias,\\n                    use_rand=quip_quant_config[\"use_rand\"],\\n                    per_channel=quip_quant_config[\"per_channel\"]\\n                )\\n                parent_name, child_name = name.rsplit(\\'.\\', 1)\\n                parent = self.get_submodule(parent_name)\\n                setattr(parent, child_name, new_module)\\n\\n    # @staticmethod\\n    # def load_weights(model: \"QuipLlamaForCausalLM\", checkpoint_path: str):\\n    #     state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\\n    #     for name, param in model.named_parameters():\\n    #         if name in state_dict:\\n    #             if isinstance(param, QuantLinear):\\n    #                 param.pack(state_dict[name], state_dict[f\"{name}_quantizer\"])\\n    #             else:\\n    #                 param.data.copy_(state_dict[name])\\n    #     model.tie_weights()\\n    def load_weights(self, checkpoint_path: str):\\n\\n        with init_empty_weights(include_buffers=False):\\n            model = AutoModelForCausalLM.from_config(\\n                self.config,\\n                trust_remote_code=True,\\n                torch_dtype=torch.float16)\\n        # model = self.model\\n            \\n        quantizer = QuipQuantizer.from_dict(quip_quant_config)\\n        model = quantizer.convert_model(model)\\n\\n        load_checkpoint_and_dispatch(\\n            model,\\n            checkpoint=checkpoint_path,\\n            device_map=\"auto\",\\n            no_split_module_classes=quantizer.get_no_split_module_classes(model),\\n            dtype=torch.float16,\\n        )\\n\\n        model.is_quantized = True\\n        model.eval()\\n        self.model = model\\n        return model\\n\\n    def forward(self, **kwargs):\\n        return self.model(**kwargs)',\n",
       " 'AutoConfig': transformers.models.auto.configuration_auto.AutoConfig,\n",
       " 'AutoModelForCausalLM': transformers.models.auto.modeling_auto.AutoModelForCausalLM,\n",
       " 'QuantLinear': qlinear.QuantLinear,\n",
       " 'E8P12RVQ4B_codebook': codebook.e8p12_rvq4.E8P12RVQ4B_codebook,\n",
       " 'Accelerator': accelerate.accelerator.Accelerator,\n",
       " 'cpu_offload_with_hook': <function accelerate.big_modeling.cpu_offload_with_hook(model: torch.nn.modules.module.Module, execution_device: Union[int, str, torch.device, NoneType] = None, prev_module_hook: Union[accelerate.hooks.UserCpuOffloadHook, NoneType] = None)>,\n",
       " 'init_empty_weights': <function accelerate.big_modeling.init_empty_weights(include_buffers: bool = None)>,\n",
       " 'load_checkpoint_and_dispatch': <function accelerate.big_modeling.load_checkpoint_and_dispatch(model: torch.nn.modules.module.Module, checkpoint: Union[str, os.PathLike], device_map: Union[str, Dict[str, Union[int, str, torch.device]], NoneType] = None, max_memory: Union[Dict[Union[int, str], Union[int, str]], NoneType] = None, no_split_module_classes: Union[List[str], NoneType] = None, offload_folder: Union[str, os.PathLike, NoneType] = None, offload_buffers: bool = False, dtype: Union[str, torch.dtype, NoneType] = None, offload_state_dict: Union[bool, NoneType] = None, skip_keys: Union[str, List[str], NoneType] = None, preload_module_classes: Union[List[str], NoneType] = None, force_hooks: bool = False, strict: bool = False)>,\n",
       " 'QuipQuantizer': quantizer.QuipQuantizer,\n",
       " 'quip_quant_config': {'quant_method': 'QUiP',\n",
       "  'rescale_WH': False,\n",
       "  'use_rand': True,\n",
       "  'codebook': 'E8P12RVQ4B',\n",
       "  'codesz': 8,\n",
       "  'idx_dtype': 'torch.int32',\n",
       "  'merge_suv': False,\n",
       "  'per_channel': False,\n",
       "  'opt_resid_scale': -1,\n",
       "  'modules_to_not_convert': None,\n",
       "  'inference': True,\n",
       "  'ft_epochs': 0},\n",
       " 'QuipLlamaForCausalLM': __main__.QuipLlamaForCausalLM,\n",
       " '_i4': 'from vllm import ModelRegistry, LLM, SamplingParams\\nfrom transformers import AutoTokenizer\\n\\n# from quip_llama import QuipLlamaForCausalLM\\nModelRegistry.register_model(\"QuipLlamaForCausalLM\", QuipLlamaForCausalLM)',\n",
       " 'ModelRegistry': vllm.model_executor.models.ModelRegistry,\n",
       " 'LLM': vllm.entrypoints.llm.LLM,\n",
       " 'SamplingParams': vllm.sampling_params.SamplingParams,\n",
       " '_i5': 'from tqdm import tqdm',\n",
       " 'tqdm': tqdm.std.tqdm,\n",
       " '_i6': '# for i in tqdm(range(10)):\\n#     pass',\n",
       " '_i7': \"model_name = 'kharinaev/Llama-3.1-8B-Instruct-quip'\",\n",
       " 'model_name': 'kharinaev/Llama-3.1-8B-Instruct-quip',\n",
       " '_i8': 'llm = LLM(\\n    model=model_name, \\n    trust_remote_code=True,\\n    enforce_eager=True,\\n)',\n",
       " '_i9': '%debug',\n",
       " '_i10': 'llm',\n",
       " '_i11': '%debug',\n",
       " '_i12': '%debug',\n",
       " '_i13': 'llm = LLM(\\n    model=model_name, \\n    trust_remote_code=True,\\n    enforce_eager=True,\\n)',\n",
       " '_i14': '%debug',\n",
       " '_i15': 'globals()',\n",
       " '_15': {...},\n",
       " '_i16': '%debug',\n",
       " '_i17': '%debug',\n",
       " '_i18': '%debug',\n",
       " '_i19': 'llm = LLM(\\n    model=model_name, \\n    trust_remote_code=True,\\n    enforce_eager=True,\\n)',\n",
       " '_i20': '%debug',\n",
       " 'weight_generator': <generator object pt_weights_iterator at 0x7fc7f022d660>,\n",
       " '_i21': 'globals()'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73542337-ace8-4580-b00e-3578bab22f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cec0e1dc9e4be4bdbf298135aebcd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/vllm/model_executor/model_loader/weight_utils.py:416: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('model.embed_tokens.weight',\n",
       " tensor([[ 0.0011,  0.0056, -0.0034,  ...,  0.0041, -0.0028, -0.0007],\n",
       "         [-0.0037,  0.0010, -0.0018,  ...,  0.0015, -0.0023, -0.0014],\n",
       "         [ 0.0014, -0.0170,  0.0032,  ...,  0.0030,  0.0095,  0.0049],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(weight_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91314843-9eca-467b-af90-9d789ec41c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
