{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcff06e2-7ec4-46ce-a9fa-04d470d1bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "os.environ[\"QUANTIZE_LLAMA_31\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7d2952-2cd9-4db6-b662-dcfb852b4af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch._custom_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b74a72-8b94-4367-b955-57002efb5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from quantizer import QuipQuantizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d7086a0-8866-42f5-8417-0327cb40f0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a575a244-ee95-4382-8e5a-145e88e9719b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b356fac-6323-40b5-ae91-fa100b637c89",
   "metadata": {},
   "source": [
    "# Llama 2 bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6bf27fe-5aa3-4e96-ad94-f40d329ce85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_name = \"VityaVitalich/Llama3.1-8b-instruct\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# quant_dir = \"/root/llm_quant_safety/quantization/models/Mistral-7B-Instruct-v0.2-quip-2bit\"\n",
    "quant_dir = \"/root/llm_quant_safety/quantization/models/Llama-3.1-8B-Instruct-quip-2bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd3c3444-55c8-4e3c-8454-24b47ada327e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccf389506e54213bd810c3510dd57db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b75694-b007-43a3-8efb-c501b65b3343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc0fa88a-84bd-4ca2-b190-c4e61d74ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def setup_logging():\n",
    "    # Настройка корневого логгера\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(),  # терминал\n",
    "            logging.FileHandler('logs/llama_e8p12.log')\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "454a2eda-c076-4588-bcb6-46517a88fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf52581f-6683-4933-bd25-2ec8c6a936c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant = QuipQuantizer(\n",
    "    # codebook=\"E8P12RVQ4B\", # 4 bit\n",
    "    codebook=\"E8P12\", # 2 bit\n",
    "    dataset=\"wikitext2\",\n",
    "    nsamples=1024, # 4096 - default ~500-750 CPU mem\n",
    "    ft_train_size=256,\n",
    "    # cache_on_gpu=True,\n",
    "    # model_seqlen=131072,\n",
    "    model_seqlen=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3f97c71-ac37-438c-9981-22565edb1c34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2436214 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c457479ed946848139387838c2c9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.layers blocks :   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 06:37:11,351 - quantizer - INFO - Start quantizing block model.layers 1/32\n",
      "2024-09-07 06:37:11,355 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 06:59:48,238 - quantizer - INFO - Quantizing self_attn.q_proj in block 1/32...\n",
      "2024-09-07 07:00:36,404 - quantizer - INFO - Quantizing self_attn.k_proj in block 1/32...\n",
      "2024-09-07 07:00:51,467 - quantizer - INFO - Quantizing self_attn.v_proj in block 1/32...\n",
      "2024-09-07 07:01:11,991 - quantizer - INFO - Block 1 initial loss 4.050996780395508\n",
      "2024-09-07 07:01:55,691 - quantizer - INFO - Quantizing self_attn.o_proj in block 1/32...\n",
      "2024-09-07 07:02:47,539 - quantizer - INFO - Block 1 initial loss 61146.3984375\n",
      "2024-09-07 07:03:29,298 - quantizer - INFO - Quantizing mlp.gate_proj in block 1/32...\n",
      "2024-09-07 07:06:05,750 - quantizer - INFO - Quantizing mlp.up_proj in block 1/32...\n",
      "2024-09-07 07:08:44,941 - quantizer - INFO - Block 1 initial loss 77009.609375\n",
      "2024-09-07 07:09:20,240 - quantizer - INFO - Quantizing mlp.down_proj in block 1/32...\n",
      "2024-09-07 07:12:04,174 - quantizer - INFO - Start quantizing block model.layers 2/32\n",
      "2024-09-07 07:12:04,177 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 07:34:42,127 - quantizer - INFO - Quantizing self_attn.q_proj in block 2/32...\n",
      "2024-09-07 07:35:28,445 - quantizer - INFO - Quantizing self_attn.k_proj in block 2/32...\n",
      "2024-09-07 07:35:42,840 - quantizer - INFO - Quantizing self_attn.v_proj in block 2/32...\n",
      "2024-09-07 07:36:02,762 - quantizer - INFO - Block 2 initial loss 2.2901103496551514\n",
      "2024-09-07 07:36:18,337 - quantizer - INFO - Block 2 @ epoch 0 new loss 2.147463321685791 old loss 2.2901103496551514 BETTER\n",
      "2024-09-07 07:36:45,236 - quantizer - INFO - Block 2 @ epoch 2 new loss 2.14357328414917 old loss 2.147463321685791 BETTER\n",
      "2024-09-07 07:36:59,320 - quantizer - INFO - Block 2 @ epoch 3 new loss 2.1386899948120117 old loss 2.14357328414917 BETTER\n",
      "2024-09-07 07:37:13,073 - quantizer - INFO - Block 2 @ epoch 4 new loss 2.123037815093994 old loss 2.1386899948120117 BETTER\n",
      "2024-09-07 07:37:14,032 - quantizer - INFO - Quantizing self_attn.o_proj in block 2/32...\n",
      "2024-09-07 07:38:06,236 - quantizer - INFO - Block 2 initial loss 23847.376953125\n",
      "2024-09-07 07:38:48,593 - quantizer - INFO - Quantizing mlp.gate_proj in block 2/32...\n",
      "2024-09-07 07:41:23,664 - quantizer - INFO - Quantizing mlp.up_proj in block 2/32...\n",
      "2024-09-07 07:44:04,278 - quantizer - INFO - Block 2 initial loss 1774356.5\n",
      "2024-09-07 07:44:41,170 - quantizer - INFO - Quantizing mlp.down_proj in block 2/32...\n",
      "2024-09-07 07:47:26,776 - quantizer - INFO - Start quantizing block model.layers 3/32\n",
      "2024-09-07 07:47:26,780 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 08:10:03,199 - quantizer - INFO - Quantizing self_attn.q_proj in block 3/32...\n",
      "2024-09-07 08:10:49,062 - quantizer - INFO - Quantizing self_attn.k_proj in block 3/32...\n",
      "2024-09-07 08:11:04,021 - quantizer - INFO - Quantizing self_attn.v_proj in block 3/32...\n",
      "2024-09-07 08:11:23,973 - quantizer - INFO - Block 3 initial loss 1.7933173179626465\n",
      "2024-09-07 08:11:38,851 - quantizer - INFO - Block 3 @ epoch 0 new loss 1.486093282699585 old loss 1.7933173179626465 BETTER\n",
      "2024-09-07 08:12:19,384 - quantizer - INFO - Quantizing self_attn.o_proj in block 3/32...\n",
      "2024-09-07 08:13:10,917 - quantizer - INFO - Block 3 initial loss 18689.6640625\n",
      "2024-09-07 08:13:26,005 - quantizer - INFO - Block 3 @ epoch 0 new loss 18418.533203125 old loss 18689.6640625 BETTER\n",
      "2024-09-07 08:13:39,578 - quantizer - INFO - Block 3 @ epoch 1 new loss 18345.875 old loss 18418.533203125 BETTER\n",
      "2024-09-07 08:13:52,952 - quantizer - INFO - Block 3 @ epoch 2 new loss 18299.2265625 old loss 18345.875 BETTER\n",
      "2024-09-07 08:14:06,988 - quantizer - INFO - Block 3 @ epoch 3 new loss 18274.94140625 old loss 18299.2265625 BETTER\n",
      "2024-09-07 08:14:19,883 - quantizer - INFO - Block 3 @ epoch 4 new loss 18255.21484375 old loss 18274.94140625 BETTER\n",
      "2024-09-07 08:14:20,739 - quantizer - INFO - Quantizing mlp.gate_proj in block 3/32...\n",
      "2024-09-07 08:16:55,601 - quantizer - INFO - Quantizing mlp.up_proj in block 3/32...\n",
      "2024-09-07 08:19:33,693 - quantizer - INFO - Block 3 initial loss 1546960.375\n",
      "2024-09-07 08:20:08,950 - quantizer - INFO - Quantizing mlp.down_proj in block 3/32...\n",
      "2024-09-07 08:22:53,279 - quantizer - INFO - Start quantizing block model.layers 4/32\n",
      "2024-09-07 08:22:53,281 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 08:45:29,636 - quantizer - INFO - Quantizing self_attn.q_proj in block 4/32...\n",
      "2024-09-07 08:46:15,688 - quantizer - INFO - Quantizing self_attn.k_proj in block 4/32...\n",
      "2024-09-07 08:46:27,738 - quantizer - INFO - Quantizing self_attn.v_proj in block 4/32...\n",
      "2024-09-07 08:46:45,119 - quantizer - INFO - Block 4 initial loss 3.278580665588379\n",
      "2024-09-07 08:47:00,306 - quantizer - INFO - Block 4 @ epoch 0 new loss 2.247908115386963 old loss 3.278580665588379 BETTER\n",
      "2024-09-07 08:47:14,610 - quantizer - INFO - Block 4 @ epoch 1 new loss 2.226945400238037 old loss 2.247908115386963 BETTER\n",
      "2024-09-07 08:47:54,142 - quantizer - INFO - Quantizing self_attn.o_proj in block 4/32...\n",
      "2024-09-07 08:48:45,057 - quantizer - INFO - Block 4 initial loss 21791.865234375\n",
      "2024-09-07 08:49:00,270 - quantizer - INFO - Block 4 @ epoch 0 new loss 21578.859375 old loss 21791.865234375 BETTER\n",
      "2024-09-07 08:49:14,292 - quantizer - INFO - Block 4 @ epoch 1 new loss 21517.62109375 old loss 21578.859375 BETTER\n",
      "2024-09-07 08:49:28,680 - quantizer - INFO - Block 4 @ epoch 2 new loss 21499.46484375 old loss 21517.62109375 BETTER\n",
      "2024-09-07 08:49:56,247 - quantizer - INFO - Quantizing mlp.gate_proj in block 4/32...\n",
      "2024-09-07 08:52:32,346 - quantizer - INFO - Quantizing mlp.up_proj in block 4/32...\n",
      "2024-09-07 08:55:11,657 - quantizer - INFO - Block 4 initial loss 1953712.25\n",
      "2024-09-07 08:55:46,385 - quantizer - INFO - Quantizing mlp.down_proj in block 4/32...\n",
      "2024-09-07 08:58:30,679 - quantizer - INFO - Start quantizing block model.layers 5/32\n",
      "2024-09-07 08:58:30,682 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 09:21:06,941 - quantizer - INFO - Quantizing self_attn.q_proj in block 5/32...\n",
      "2024-09-07 09:21:53,714 - quantizer - INFO - Quantizing self_attn.k_proj in block 5/32...\n",
      "2024-09-07 09:22:08,126 - quantizer - INFO - Quantizing self_attn.v_proj in block 5/32...\n",
      "2024-09-07 09:22:28,572 - quantizer - INFO - Block 5 initial loss 5.12774133682251\n",
      "2024-09-07 09:22:42,729 - quantizer - INFO - Block 5 @ epoch 0 new loss 4.263970851898193 old loss 5.12774133682251 BETTER\n",
      "2024-09-07 09:22:56,477 - quantizer - INFO - Block 5 @ epoch 1 new loss 4.2508225440979 old loss 4.263970851898193 BETTER\n",
      "2024-09-07 09:23:36,885 - quantizer - INFO - Quantizing self_attn.o_proj in block 5/32...\n",
      "2024-09-07 09:24:28,564 - quantizer - INFO - Block 5 initial loss 40458.80859375\n",
      "2024-09-07 09:24:43,687 - quantizer - INFO - Block 5 @ epoch 0 new loss 40001.76171875 old loss 40458.80859375 BETTER\n",
      "2024-09-07 09:24:58,338 - quantizer - INFO - Block 5 @ epoch 1 new loss 39749.48828125 old loss 40001.76171875 BETTER\n",
      "2024-09-07 09:25:11,744 - quantizer - INFO - Block 5 @ epoch 2 new loss 39665.1171875 old loss 39749.48828125 BETTER\n",
      "2024-09-07 09:25:25,084 - quantizer - INFO - Block 5 @ epoch 3 new loss 39661.53125 old loss 39665.1171875 BETTER\n",
      "2024-09-07 09:25:39,557 - quantizer - INFO - Block 5 @ epoch 4 new loss 39649.3984375 old loss 39661.53125 BETTER\n",
      "2024-09-07 09:25:40,555 - quantizer - INFO - Quantizing mlp.gate_proj in block 5/32...\n",
      "2024-09-07 09:28:15,909 - quantizer - INFO - Quantizing mlp.up_proj in block 5/32...\n",
      "2024-09-07 09:30:55,371 - quantizer - INFO - Block 5 initial loss 1930564.0\n",
      "2024-09-07 09:31:31,722 - quantizer - INFO - Quantizing mlp.down_proj in block 5/32...\n",
      "2024-09-07 09:34:16,511 - quantizer - INFO - Start quantizing block model.layers 6/32\n",
      "2024-09-07 09:34:16,515 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 09:56:53,202 - quantizer - INFO - Quantizing self_attn.q_proj in block 6/32...\n",
      "2024-09-07 09:57:38,920 - quantizer - INFO - Quantizing self_attn.k_proj in block 6/32...\n",
      "2024-09-07 09:57:53,847 - quantizer - INFO - Quantizing self_attn.v_proj in block 6/32...\n",
      "2024-09-07 09:58:13,224 - quantizer - INFO - Block 6 initial loss 16.149049758911133\n",
      "2024-09-07 09:58:28,122 - quantizer - INFO - Block 6 @ epoch 0 new loss 13.702676773071289 old loss 16.149049758911133 BETTER\n",
      "2024-09-07 09:58:40,684 - quantizer - INFO - Block 6 @ epoch 1 new loss 13.643633842468262 old loss 13.702676773071289 BETTER\n",
      "2024-09-07 09:58:55,023 - quantizer - INFO - Block 6 @ epoch 2 new loss 13.627815246582031 old loss 13.643633842468262 BETTER\n",
      "2024-09-07 09:59:09,003 - quantizer - INFO - Block 6 @ epoch 3 new loss 13.55597972869873 old loss 13.627815246582031 BETTER\n",
      "2024-09-07 09:59:23,097 - quantizer - INFO - Quantizing self_attn.o_proj in block 6/32...\n",
      "2024-09-07 10:00:14,509 - quantizer - INFO - Block 6 initial loss 151264.15625\n",
      "2024-09-07 10:00:30,126 - quantizer - INFO - Block 6 @ epoch 0 new loss 150776.109375 old loss 151264.15625 BETTER\n",
      "2024-09-07 10:00:44,666 - quantizer - INFO - Block 6 @ epoch 1 new loss 150396.09375 old loss 150776.109375 BETTER\n",
      "2024-09-07 10:01:24,933 - quantizer - INFO - Quantizing mlp.gate_proj in block 6/32...\n",
      "2024-09-07 10:04:01,409 - quantizer - INFO - Quantizing mlp.up_proj in block 6/32...\n",
      "2024-09-07 10:06:40,875 - quantizer - INFO - Block 6 initial loss 5301772.0\n",
      "2024-09-07 10:07:19,607 - quantizer - INFO - Quantizing mlp.down_proj in block 6/32...\n",
      "2024-09-07 10:10:04,492 - quantizer - INFO - Start quantizing block model.layers 7/32\n",
      "2024-09-07 10:10:04,494 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 10:32:41,467 - quantizer - INFO - Quantizing self_attn.q_proj in block 7/32...\n",
      "2024-09-07 10:33:27,581 - quantizer - INFO - Quantizing self_attn.k_proj in block 7/32...\n",
      "2024-09-07 10:33:39,691 - quantizer - INFO - Quantizing self_attn.v_proj in block 7/32...\n",
      "2024-09-07 10:33:56,914 - quantizer - INFO - Block 7 initial loss 22.291034698486328\n",
      "2024-09-07 10:34:12,800 - quantizer - INFO - Block 7 @ epoch 0 new loss 19.229650497436523 old loss 22.291034698486328 BETTER\n",
      "2024-09-07 10:34:26,025 - quantizer - INFO - Block 7 @ epoch 1 new loss 19.185131072998047 old loss 19.229650497436523 BETTER\n",
      "2024-09-07 10:35:07,304 - quantizer - INFO - Quantizing self_attn.o_proj in block 7/32...\n",
      "2024-09-07 10:35:59,591 - quantizer - INFO - Block 7 initial loss 195260.046875\n",
      "2024-09-07 10:36:14,222 - quantizer - INFO - Block 7 @ epoch 0 new loss 192856.625 old loss 195260.046875 BETTER\n",
      "2024-09-07 10:36:53,316 - quantizer - INFO - Quantizing mlp.gate_proj in block 7/32...\n",
      "2024-09-07 10:39:28,184 - quantizer - INFO - Quantizing mlp.up_proj in block 7/32...\n",
      "2024-09-07 10:42:07,753 - quantizer - INFO - Block 7 initial loss 7606116.5\n",
      "2024-09-07 10:42:44,710 - quantizer - INFO - Quantizing mlp.down_proj in block 7/32...\n",
      "2024-09-07 10:45:29,066 - quantizer - INFO - Start quantizing block model.layers 8/32\n",
      "2024-09-07 10:45:29,067 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 11:08:05,483 - quantizer - INFO - Quantizing self_attn.q_proj in block 8/32...\n",
      "2024-09-07 11:08:51,193 - quantizer - INFO - Quantizing self_attn.k_proj in block 8/32...\n",
      "2024-09-07 11:09:06,057 - quantizer - INFO - Quantizing self_attn.v_proj in block 8/32...\n",
      "2024-09-07 11:09:26,520 - quantizer - INFO - Block 8 initial loss 32.36016082763672\n",
      "2024-09-07 11:09:40,931 - quantizer - INFO - Block 8 @ epoch 0 new loss 30.77618408203125 old loss 32.36016082763672 BETTER\n",
      "2024-09-07 11:09:54,295 - quantizer - INFO - Block 8 @ epoch 1 new loss 30.725065231323242 old loss 30.77618408203125 BETTER\n",
      "2024-09-07 11:10:34,474 - quantizer - INFO - Quantizing self_attn.o_proj in block 8/32...\n",
      "2024-09-07 11:11:26,139 - quantizer - INFO - Block 8 initial loss 304333.4375\n",
      "2024-09-07 11:11:41,068 - quantizer - INFO - Block 8 @ epoch 0 new loss 299015.75 old loss 304333.4375 BETTER\n",
      "2024-09-07 11:12:21,416 - quantizer - INFO - Quantizing mlp.gate_proj in block 8/32...\n",
      "2024-09-07 11:14:56,974 - quantizer - INFO - Quantizing mlp.up_proj in block 8/32...\n",
      "2024-09-07 11:17:37,033 - quantizer - INFO - Block 8 initial loss 4981252.0\n",
      "2024-09-07 11:18:14,303 - quantizer - INFO - Quantizing mlp.down_proj in block 8/32...\n",
      "2024-09-07 11:21:00,029 - quantizer - INFO - Start quantizing block model.layers 9/32\n",
      "2024-09-07 11:21:00,033 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 11:43:36,431 - quantizer - INFO - Quantizing self_attn.q_proj in block 9/32...\n",
      "2024-09-07 11:44:23,295 - quantizer - INFO - Quantizing self_attn.k_proj in block 9/32...\n",
      "2024-09-07 11:44:37,933 - quantizer - INFO - Quantizing self_attn.v_proj in block 9/32...\n",
      "2024-09-07 11:44:58,792 - quantizer - INFO - Block 9 initial loss 38.34006118774414\n",
      "2024-09-07 11:45:15,084 - quantizer - INFO - Block 9 @ epoch 0 new loss 36.744041442871094 old loss 38.34006118774414 BETTER\n",
      "2024-09-07 11:45:29,147 - quantizer - INFO - Block 9 @ epoch 1 new loss 36.56977081298828 old loss 36.744041442871094 BETTER\n",
      "2024-09-07 11:46:10,483 - quantizer - INFO - Quantizing self_attn.o_proj in block 9/32...\n",
      "2024-09-07 11:47:02,182 - quantizer - INFO - Block 9 initial loss 343466.4375\n",
      "2024-09-07 11:47:16,376 - quantizer - INFO - Block 9 @ epoch 0 new loss 336104.28125 old loss 343466.4375 BETTER\n",
      "2024-09-07 11:47:55,872 - quantizer - INFO - Quantizing mlp.gate_proj in block 9/32...\n",
      "2024-09-07 11:50:30,287 - quantizer - INFO - Quantizing mlp.up_proj in block 9/32...\n",
      "2024-09-07 11:53:09,307 - quantizer - INFO - Block 9 initial loss 4056999.25\n",
      "2024-09-07 11:53:45,512 - quantizer - INFO - Quantizing mlp.down_proj in block 9/32...\n",
      "2024-09-07 11:56:30,422 - quantizer - INFO - Start quantizing block model.layers 10/32\n",
      "2024-09-07 11:56:30,425 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 12:19:06,247 - quantizer - INFO - Quantizing self_attn.q_proj in block 10/32...\n",
      "2024-09-07 12:19:53,190 - quantizer - INFO - Quantizing self_attn.k_proj in block 10/32...\n",
      "2024-09-07 12:20:07,950 - quantizer - INFO - Quantizing self_attn.v_proj in block 10/32...\n",
      "2024-09-07 12:20:28,521 - quantizer - INFO - Block 10 initial loss 36.63201141357422\n",
      "2024-09-07 12:20:43,266 - quantizer - INFO - Block 10 @ epoch 0 new loss 35.6436767578125 old loss 36.63201141357422 BETTER\n",
      "2024-09-07 12:20:57,076 - quantizer - INFO - Block 10 @ epoch 1 new loss 35.257713317871094 old loss 35.6436767578125 BETTER\n",
      "2024-09-07 12:21:39,818 - quantizer - INFO - Quantizing self_attn.o_proj in block 10/32...\n",
      "2024-09-07 12:22:31,050 - quantizer - INFO - Block 10 initial loss 292803.0625\n",
      "2024-09-07 12:22:45,558 - quantizer - INFO - Block 10 @ epoch 0 new loss 283292.53125 old loss 292803.0625 BETTER\n",
      "2024-09-07 12:23:24,589 - quantizer - INFO - Quantizing mlp.gate_proj in block 10/32...\n",
      "2024-09-07 12:25:59,802 - quantizer - INFO - Quantizing mlp.up_proj in block 10/32...\n",
      "2024-09-07 12:28:38,290 - quantizer - INFO - Block 10 initial loss 4223418.0\n",
      "2024-09-07 12:29:15,362 - quantizer - INFO - Block 10 @ epoch 2 new loss 4217017.0 old loss 4223418.0 BETTER\n",
      "2024-09-07 12:29:25,922 - quantizer - INFO - Block 10 @ epoch 3 new loss 4129706.5 old loss 4217017.0 BETTER\n",
      "2024-09-07 12:29:36,985 - quantizer - INFO - Block 10 @ epoch 4 new loss 4057867.5 old loss 4129706.5 BETTER\n",
      "2024-09-07 12:29:37,171 - quantizer - INFO - Quantizing mlp.down_proj in block 10/32...\n",
      "2024-09-07 12:32:22,206 - quantizer - INFO - Start quantizing block model.layers 11/32\n",
      "2024-09-07 12:32:22,209 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 12:54:58,505 - quantizer - INFO - Quantizing self_attn.q_proj in block 11/32...\n",
      "2024-09-07 12:55:45,056 - quantizer - INFO - Quantizing self_attn.k_proj in block 11/32...\n",
      "2024-09-07 12:55:59,503 - quantizer - INFO - Quantizing self_attn.v_proj in block 11/32...\n",
      "2024-09-07 12:56:20,256 - quantizer - INFO - Block 11 initial loss 46.04235076904297\n",
      "2024-09-07 12:56:35,188 - quantizer - INFO - Block 11 @ epoch 0 new loss 41.04916000366211 old loss 46.04235076904297 BETTER\n",
      "2024-09-07 12:57:15,586 - quantizer - INFO - Quantizing self_attn.o_proj in block 11/32...\n",
      "2024-09-07 12:58:07,316 - quantizer - INFO - Block 11 initial loss 392834.21875\n",
      "2024-09-07 12:58:22,505 - quantizer - INFO - Block 11 @ epoch 0 new loss 386747.78125 old loss 392834.21875 BETTER\n",
      "2024-09-07 12:59:03,856 - quantizer - INFO - Quantizing mlp.gate_proj in block 11/32...\n",
      "2024-09-07 13:01:38,194 - quantizer - INFO - Quantizing mlp.up_proj in block 11/32...\n",
      "2024-09-07 13:04:18,117 - quantizer - INFO - Block 11 initial loss 4674879.0\n",
      "2024-09-07 13:04:54,231 - quantizer - INFO - Quantizing mlp.down_proj in block 11/32...\n",
      "2024-09-07 13:07:39,293 - quantizer - INFO - Start quantizing block model.layers 12/32\n",
      "2024-09-07 13:07:39,295 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 13:30:15,632 - quantizer - INFO - Quantizing self_attn.q_proj in block 12/32...\n",
      "2024-09-07 13:31:00,896 - quantizer - INFO - Quantizing self_attn.k_proj in block 12/32...\n",
      "2024-09-07 13:31:14,424 - quantizer - INFO - Quantizing self_attn.v_proj in block 12/32...\n",
      "2024-09-07 13:31:34,989 - quantizer - INFO - Block 12 initial loss 63.71611022949219\n",
      "2024-09-07 13:31:50,669 - quantizer - INFO - Block 12 @ epoch 0 new loss 62.752113342285156 old loss 63.71611022949219 BETTER\n",
      "2024-09-07 13:32:05,538 - quantizer - INFO - Block 12 @ epoch 1 new loss 62.4347038269043 old loss 62.752113342285156 BETTER\n",
      "2024-09-07 13:32:47,051 - quantizer - INFO - Quantizing self_attn.o_proj in block 12/32...\n",
      "2024-09-07 13:33:39,685 - quantizer - INFO - Block 12 initial loss 581198.75\n",
      "2024-09-07 13:33:55,849 - quantizer - INFO - Block 12 @ epoch 0 new loss 552305.375 old loss 581198.75 BETTER\n",
      "2024-09-07 13:34:21,788 - quantizer - INFO - Block 12 @ epoch 2 new loss 547137.1875 old loss 552305.375 BETTER\n",
      "2024-09-07 13:34:48,273 - quantizer - INFO - Quantizing mlp.gate_proj in block 12/32...\n",
      "2024-09-07 13:37:23,182 - quantizer - INFO - Quantizing mlp.up_proj in block 12/32...\n",
      "2024-09-07 13:40:03,170 - quantizer - INFO - Block 12 initial loss 4859530.0\n",
      "2024-09-07 13:40:42,531 - quantizer - INFO - Quantizing mlp.down_proj in block 12/32...\n",
      "2024-09-07 13:43:28,621 - quantizer - INFO - Start quantizing block model.layers 13/32\n",
      "2024-09-07 13:43:28,622 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 14:06:04,804 - quantizer - INFO - Quantizing self_attn.q_proj in block 13/32...\n",
      "2024-09-07 14:06:51,619 - quantizer - INFO - Quantizing self_attn.k_proj in block 13/32...\n",
      "2024-09-07 14:07:06,111 - quantizer - INFO - Quantizing self_attn.v_proj in block 13/32...\n",
      "2024-09-07 14:07:26,786 - quantizer - INFO - Block 13 initial loss 61.97264099121094\n",
      "2024-09-07 14:07:42,880 - quantizer - INFO - Block 13 @ epoch 0 new loss 61.04629135131836 old loss 61.97264099121094 BETTER\n",
      "2024-09-07 14:08:23,147 - quantizer - INFO - Quantizing self_attn.o_proj in block 13/32...\n",
      "2024-09-07 14:09:15,015 - quantizer - INFO - Block 13 initial loss 507139.40625\n",
      "2024-09-07 14:09:30,401 - quantizer - INFO - Block 13 @ epoch 0 new loss 484231.03125 old loss 507139.40625 BETTER\n",
      "2024-09-07 14:10:11,318 - quantizer - INFO - Quantizing mlp.gate_proj in block 13/32...\n",
      "2024-09-07 14:12:45,254 - quantizer - INFO - Quantizing mlp.up_proj in block 13/32...\n",
      "2024-09-07 14:15:25,323 - quantizer - INFO - Block 13 initial loss 4999777.5\n",
      "2024-09-07 14:16:00,822 - quantizer - INFO - Block 13 @ epoch 2 new loss 4955949.0 old loss 4999777.5 BETTER\n",
      "2024-09-07 14:16:11,500 - quantizer - INFO - Block 13 @ epoch 3 new loss 4861886.5 old loss 4955949.0 BETTER\n",
      "2024-09-07 14:16:22,403 - quantizer - INFO - Block 13 @ epoch 4 new loss 4787504.5 old loss 4861886.5 BETTER\n",
      "2024-09-07 14:16:22,621 - quantizer - INFO - Quantizing mlp.down_proj in block 13/32...\n",
      "2024-09-07 14:19:07,733 - quantizer - INFO - Start quantizing block model.layers 14/32\n",
      "2024-09-07 14:19:07,735 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 14:41:45,591 - quantizer - INFO - Quantizing self_attn.q_proj in block 14/32...\n",
      "2024-09-07 14:42:31,613 - quantizer - INFO - Quantizing self_attn.k_proj in block 14/32...\n",
      "2024-09-07 14:42:46,417 - quantizer - INFO - Quantizing self_attn.v_proj in block 14/32...\n",
      "2024-09-07 14:43:04,331 - quantizer - INFO - Block 14 initial loss 88.36815643310547\n",
      "2024-09-07 14:43:21,055 - quantizer - INFO - Block 14 @ epoch 0 new loss 85.35791778564453 old loss 88.36815643310547 BETTER\n",
      "2024-09-07 14:44:03,004 - quantizer - INFO - Quantizing self_attn.o_proj in block 14/32...\n",
      "2024-09-07 14:44:55,911 - quantizer - INFO - Block 14 initial loss 741533.3125\n",
      "2024-09-07 14:45:11,210 - quantizer - INFO - Block 14 @ epoch 0 new loss 723369.0 old loss 741533.3125 BETTER\n",
      "2024-09-07 14:45:51,748 - quantizer - INFO - Quantizing mlp.gate_proj in block 14/32...\n",
      "2024-09-07 14:48:26,676 - quantizer - INFO - Quantizing mlp.up_proj in block 14/32...\n",
      "2024-09-07 14:51:06,907 - quantizer - INFO - Block 14 initial loss 3288685.0\n",
      "2024-09-07 14:51:31,160 - quantizer - INFO - Block 14 @ epoch 1 new loss 3258432.75 old loss 3288685.0 BETTER\n",
      "2024-09-07 14:51:43,695 - quantizer - INFO - Block 14 @ epoch 2 new loss 3216284.0 old loss 3258432.75 BETTER\n",
      "2024-09-07 14:51:55,551 - quantizer - INFO - Block 14 @ epoch 3 new loss 3184025.75 old loss 3216284.0 BETTER\n",
      "2024-09-07 14:52:06,224 - quantizer - INFO - Block 14 @ epoch 4 new loss 3159080.25 old loss 3184025.75 BETTER\n",
      "2024-09-07 14:52:06,578 - quantizer - INFO - Quantizing mlp.down_proj in block 14/32...\n",
      "2024-09-07 14:54:52,196 - quantizer - INFO - Start quantizing block model.layers 15/32\n",
      "2024-09-07 14:54:52,201 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 15:17:30,563 - quantizer - INFO - Quantizing self_attn.q_proj in block 15/32...\n",
      "2024-09-07 15:18:16,398 - quantizer - INFO - Quantizing self_attn.k_proj in block 15/32...\n",
      "2024-09-07 15:18:31,121 - quantizer - INFO - Quantizing self_attn.v_proj in block 15/32...\n",
      "2024-09-07 15:18:51,768 - quantizer - INFO - Block 15 initial loss 82.58631896972656\n",
      "2024-09-07 15:19:34,750 - quantizer - INFO - Quantizing self_attn.o_proj in block 15/32...\n",
      "2024-09-07 15:20:26,649 - quantizer - INFO - Block 15 initial loss 732200.875\n",
      "2024-09-07 15:21:07,750 - quantizer - INFO - Quantizing mlp.gate_proj in block 15/32...\n",
      "2024-09-07 15:23:42,609 - quantizer - INFO - Quantizing mlp.up_proj in block 15/32...\n",
      "2024-09-07 15:26:21,676 - quantizer - INFO - Block 15 initial loss 2811417.75\n",
      "2024-09-07 15:26:35,025 - quantizer - INFO - Block 15 @ epoch 0 new loss 2637198.5 old loss 2811417.75 BETTER\n",
      "2024-09-07 15:26:46,675 - quantizer - INFO - Block 15 @ epoch 1 new loss 2540465.0 old loss 2637198.5 BETTER\n",
      "2024-09-07 15:26:57,455 - quantizer - INFO - Block 15 @ epoch 2 new loss 2476239.5 old loss 2540465.0 BETTER\n",
      "2024-09-07 15:27:08,147 - quantizer - INFO - Block 15 @ epoch 3 new loss 2434705.0 old loss 2476239.5 BETTER\n",
      "2024-09-07 15:27:20,031 - quantizer - INFO - Block 15 @ epoch 4 new loss 2404552.25 old loss 2434705.0 BETTER\n",
      "2024-09-07 15:27:20,222 - quantizer - INFO - Quantizing mlp.down_proj in block 15/32...\n",
      "2024-09-07 15:30:04,539 - quantizer - INFO - Start quantizing block model.layers 16/32\n",
      "2024-09-07 15:30:04,546 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 15:52:43,374 - quantizer - INFO - Quantizing self_attn.q_proj in block 16/32...\n",
      "2024-09-07 15:53:30,442 - quantizer - INFO - Quantizing self_attn.k_proj in block 16/32...\n",
      "2024-09-07 15:53:44,756 - quantizer - INFO - Quantizing self_attn.v_proj in block 16/32...\n",
      "2024-09-07 15:54:04,719 - quantizer - INFO - Block 16 initial loss 58.34549331665039\n",
      "2024-09-07 15:54:19,743 - quantizer - INFO - Block 16 @ epoch 0 new loss 58.11836624145508 old loss 58.34549331665039 BETTER\n",
      "2024-09-07 15:54:34,238 - quantizer - INFO - Block 16 @ epoch 1 new loss 57.80389404296875 old loss 58.11836624145508 BETTER\n",
      "2024-09-07 15:54:48,605 - quantizer - INFO - Block 16 @ epoch 2 new loss 56.842201232910156 old loss 57.80389404296875 BETTER\n",
      "2024-09-07 15:55:16,879 - quantizer - INFO - Quantizing self_attn.o_proj in block 16/32...\n",
      "2024-09-07 15:56:08,552 - quantizer - INFO - Block 16 initial loss 456546.21875\n",
      "2024-09-07 15:56:24,086 - quantizer - INFO - Block 16 @ epoch 0 new loss 447808.625 old loss 456546.21875 BETTER\n",
      "2024-09-07 15:57:06,458 - quantizer - INFO - Quantizing mlp.gate_proj in block 16/32...\n",
      "2024-09-07 15:59:41,473 - quantizer - INFO - Quantizing mlp.up_proj in block 16/32...\n",
      "2024-09-07 16:02:20,150 - quantizer - INFO - Block 16 initial loss 5352797.5\n",
      "2024-09-07 16:02:58,215 - quantizer - INFO - Quantizing mlp.down_proj in block 16/32...\n",
      "2024-09-07 16:05:42,906 - quantizer - INFO - Start quantizing block model.layers 17/32\n",
      "2024-09-07 16:05:42,914 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 16:28:21,744 - quantizer - INFO - Quantizing self_attn.q_proj in block 17/32...\n",
      "2024-09-07 16:29:08,287 - quantizer - INFO - Quantizing self_attn.k_proj in block 17/32...\n",
      "2024-09-07 16:29:22,570 - quantizer - INFO - Quantizing self_attn.v_proj in block 17/32...\n",
      "2024-09-07 16:29:43,643 - quantizer - INFO - Block 17 initial loss 58.94234848022461\n",
      "2024-09-07 16:30:26,263 - quantizer - INFO - Quantizing self_attn.o_proj in block 17/32...\n",
      "2024-09-07 16:31:18,437 - quantizer - INFO - Block 17 initial loss 507187.09375\n",
      "2024-09-07 16:31:59,761 - quantizer - INFO - Quantizing mlp.gate_proj in block 17/32...\n",
      "2024-09-07 16:34:35,211 - quantizer - INFO - Quantizing mlp.up_proj in block 17/32...\n",
      "2024-09-07 16:37:15,016 - quantizer - INFO - Block 17 initial loss 3644023.75\n",
      "2024-09-07 16:37:51,368 - quantizer - INFO - Quantizing mlp.down_proj in block 17/32...\n",
      "2024-09-07 16:40:33,883 - quantizer - INFO - Start quantizing block model.layers 18/32\n",
      "2024-09-07 16:40:33,891 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 17:03:13,480 - quantizer - INFO - Quantizing self_attn.q_proj in block 18/32...\n",
      "2024-09-07 17:04:00,219 - quantizer - INFO - Quantizing self_attn.k_proj in block 18/32...\n",
      "2024-09-07 17:04:14,466 - quantizer - INFO - Quantizing self_attn.v_proj in block 18/32...\n",
      "2024-09-07 17:04:34,922 - quantizer - INFO - Block 18 initial loss 38.21002197265625\n",
      "2024-09-07 17:04:49,091 - quantizer - INFO - Block 18 @ epoch 0 new loss 36.477325439453125 old loss 38.21002197265625 BETTER\n",
      "2024-09-07 17:05:02,335 - quantizer - INFO - Block 18 @ epoch 1 new loss 35.76276397705078 old loss 36.477325439453125 BETTER\n",
      "2024-09-07 17:05:17,188 - quantizer - INFO - Block 18 @ epoch 2 new loss 34.882469177246094 old loss 35.76276397705078 BETTER\n",
      "2024-09-07 17:05:30,470 - quantizer - INFO - Block 18 @ epoch 3 new loss 33.56580352783203 old loss 34.882469177246094 BETTER\n",
      "2024-09-07 17:05:44,509 - quantizer - INFO - Quantizing self_attn.o_proj in block 18/32...\n",
      "2024-09-07 17:06:36,074 - quantizer - INFO - Block 18 initial loss 264334.125\n",
      "2024-09-07 17:07:18,615 - quantizer - INFO - Quantizing mlp.gate_proj in block 18/32...\n",
      "2024-09-07 17:09:51,902 - quantizer - INFO - Quantizing mlp.up_proj in block 18/32...\n",
      "2024-09-07 17:12:31,993 - quantizer - INFO - Block 18 initial loss 11745937.0\n",
      "2024-09-07 17:13:07,663 - quantizer - INFO - Quantizing mlp.down_proj in block 18/32...\n",
      "2024-09-07 17:15:53,509 - quantizer - INFO - Start quantizing block model.layers 19/32\n",
      "2024-09-07 17:15:53,517 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 17:38:32,051 - quantizer - INFO - Quantizing self_attn.q_proj in block 19/32...\n",
      "2024-09-07 17:39:18,131 - quantizer - INFO - Quantizing self_attn.k_proj in block 19/32...\n",
      "2024-09-07 17:39:32,455 - quantizer - INFO - Quantizing self_attn.v_proj in block 19/32...\n",
      "2024-09-07 17:39:53,108 - quantizer - INFO - Block 19 initial loss 25.953901290893555\n",
      "2024-09-07 17:40:08,835 - quantizer - INFO - Block 19 @ epoch 0 new loss 25.406776428222656 old loss 25.953901290893555 BETTER\n",
      "2024-09-07 17:40:22,074 - quantizer - INFO - Block 19 @ epoch 1 new loss 25.325895309448242 old loss 25.406776428222656 BETTER\n",
      "2024-09-07 17:40:35,652 - quantizer - INFO - Block 19 @ epoch 2 new loss 25.108531951904297 old loss 25.325895309448242 BETTER\n",
      "2024-09-07 17:40:49,501 - quantizer - INFO - Block 19 @ epoch 3 new loss 25.04026985168457 old loss 25.108531951904297 BETTER\n",
      "2024-09-07 17:41:02,559 - quantizer - INFO - Block 19 @ epoch 4 new loss 24.876636505126953 old loss 25.04026985168457 BETTER\n",
      "2024-09-07 17:41:03,100 - quantizer - INFO - Quantizing self_attn.o_proj in block 19/32...\n",
      "2024-09-07 17:41:54,895 - quantizer - INFO - Block 19 initial loss 204398.3125\n",
      "2024-09-07 17:42:36,307 - quantizer - INFO - Quantizing mlp.gate_proj in block 19/32...\n",
      "2024-09-07 17:45:12,223 - quantizer - INFO - Quantizing mlp.up_proj in block 19/32...\n",
      "2024-09-07 17:47:51,776 - quantizer - INFO - Block 19 initial loss 20174576.0\n",
      "2024-09-07 17:48:30,857 - quantizer - INFO - Quantizing mlp.down_proj in block 19/32...\n",
      "2024-09-07 17:51:14,758 - quantizer - INFO - Start quantizing block model.layers 20/32\n",
      "2024-09-07 17:51:14,761 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 18:13:54,282 - quantizer - INFO - Quantizing self_attn.q_proj in block 20/32...\n",
      "2024-09-07 18:14:40,332 - quantizer - INFO - Quantizing self_attn.k_proj in block 20/32...\n",
      "2024-09-07 18:14:55,166 - quantizer - INFO - Quantizing self_attn.v_proj in block 20/32...\n",
      "2024-09-07 18:15:15,702 - quantizer - INFO - Block 20 initial loss 18.049497604370117\n",
      "2024-09-07 18:15:32,471 - quantizer - INFO - Block 20 @ epoch 0 new loss 16.76055145263672 old loss 18.049497604370117 BETTER\n",
      "2024-09-07 18:15:48,228 - quantizer - INFO - Block 20 @ epoch 1 new loss 16.679536819458008 old loss 16.76055145263672 BETTER\n",
      "2024-09-07 18:16:01,596 - quantizer - INFO - Block 20 @ epoch 2 new loss 16.63874053955078 old loss 16.679536819458008 BETTER\n",
      "2024-09-07 18:16:15,595 - quantizer - INFO - Block 20 @ epoch 3 new loss 16.566240310668945 old loss 16.63874053955078 BETTER\n",
      "2024-09-07 18:16:30,618 - quantizer - INFO - Block 20 @ epoch 4 new loss 16.530948638916016 old loss 16.566240310668945 BETTER\n",
      "2024-09-07 18:16:31,446 - quantizer - INFO - Quantizing self_attn.o_proj in block 20/32...\n",
      "2024-09-07 18:17:22,504 - quantizer - INFO - Block 20 initial loss 134544.53125\n",
      "2024-09-07 18:18:05,406 - quantizer - INFO - Quantizing mlp.gate_proj in block 20/32...\n",
      "2024-09-07 18:20:39,133 - quantizer - INFO - Quantizing mlp.up_proj in block 20/32...\n",
      "2024-09-07 18:23:18,469 - quantizer - INFO - Block 20 initial loss 14083259.0\n",
      "2024-09-07 18:23:56,584 - quantizer - INFO - Quantizing mlp.down_proj in block 20/32...\n",
      "2024-09-07 18:26:40,710 - quantizer - INFO - Start quantizing block model.layers 21/32\n",
      "2024-09-07 18:26:40,712 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 18:49:19,665 - quantizer - INFO - Quantizing self_attn.q_proj in block 21/32...\n",
      "2024-09-07 18:50:06,258 - quantizer - INFO - Quantizing self_attn.k_proj in block 21/32...\n",
      "2024-09-07 18:50:20,817 - quantizer - INFO - Quantizing self_attn.v_proj in block 21/32...\n",
      "2024-09-07 18:50:41,865 - quantizer - INFO - Block 21 initial loss 17.55139923095703\n",
      "2024-09-07 18:50:56,961 - quantizer - INFO - Block 21 @ epoch 0 new loss 14.865346908569336 old loss 17.55139923095703 BETTER\n",
      "2024-09-07 18:51:11,337 - quantizer - INFO - Block 21 @ epoch 1 new loss 14.706883430480957 old loss 14.865346908569336 BETTER\n",
      "2024-09-07 18:51:25,225 - quantizer - INFO - Block 21 @ epoch 2 new loss 14.635638236999512 old loss 14.706883430480957 BETTER\n",
      "2024-09-07 18:51:37,877 - quantizer - INFO - Block 21 @ epoch 3 new loss 14.592581748962402 old loss 14.635638236999512 BETTER\n",
      "2024-09-07 18:51:52,212 - quantizer - INFO - Block 21 @ epoch 4 new loss 14.575891494750977 old loss 14.592581748962402 BETTER\n",
      "2024-09-07 18:51:53,236 - quantizer - INFO - Quantizing self_attn.o_proj in block 21/32...\n",
      "2024-09-07 18:52:44,443 - quantizer - INFO - Block 21 initial loss 123776.390625\n",
      "2024-09-07 18:52:59,372 - quantizer - INFO - Block 21 @ epoch 0 new loss 122571.9609375 old loss 123776.390625 BETTER\n",
      "2024-09-07 18:53:12,473 - quantizer - INFO - Block 21 @ epoch 1 new loss 122081.203125 old loss 122571.9609375 BETTER\n",
      "2024-09-07 18:53:54,146 - quantizer - INFO - Quantizing mlp.gate_proj in block 21/32...\n",
      "2024-09-07 18:56:28,575 - quantizer - INFO - Quantizing mlp.up_proj in block 21/32...\n",
      "2024-09-07 18:59:07,288 - quantizer - INFO - Block 21 initial loss 9235924.0\n",
      "2024-09-07 18:59:43,153 - quantizer - INFO - Quantizing mlp.down_proj in block 21/32...\n",
      "2024-09-07 19:02:26,078 - quantizer - INFO - Start quantizing block model.layers 22/32\n",
      "2024-09-07 19:02:26,080 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 19:25:04,645 - quantizer - INFO - Quantizing self_attn.q_proj in block 22/32...\n",
      "2024-09-07 19:25:50,143 - quantizer - INFO - Quantizing self_attn.k_proj in block 22/32...\n",
      "2024-09-07 19:26:04,886 - quantizer - INFO - Quantizing self_attn.v_proj in block 22/32...\n",
      "2024-09-07 19:26:24,812 - quantizer - INFO - Block 22 initial loss 35.55943298339844\n",
      "2024-09-07 19:26:41,370 - quantizer - INFO - Block 22 @ epoch 0 new loss 30.841632843017578 old loss 35.55943298339844 BETTER\n",
      "2024-09-07 19:26:54,429 - quantizer - INFO - Block 22 @ epoch 1 new loss 30.61009979248047 old loss 30.841632843017578 BETTER\n",
      "2024-09-07 19:27:07,826 - quantizer - INFO - Block 22 @ epoch 2 new loss 29.70191192626953 old loss 30.61009979248047 BETTER\n",
      "2024-09-07 19:27:35,172 - quantizer - INFO - Quantizing self_attn.o_proj in block 22/32...\n",
      "2024-09-07 19:28:27,574 - quantizer - INFO - Block 22 initial loss 241669.84375\n",
      "2024-09-07 19:28:43,818 - quantizer - INFO - Block 22 @ epoch 0 new loss 241306.6875 old loss 241669.84375 BETTER\n",
      "2024-09-07 19:29:23,850 - quantizer - INFO - Quantizing mlp.gate_proj in block 22/32...\n",
      "2024-09-07 19:31:59,162 - quantizer - INFO - Quantizing mlp.up_proj in block 22/32...\n",
      "2024-09-07 19:34:38,402 - quantizer - INFO - Block 22 initial loss 5755064.5\n",
      "2024-09-07 19:35:14,009 - quantizer - INFO - Quantizing mlp.down_proj in block 22/32...\n",
      "2024-09-07 19:37:59,391 - quantizer - INFO - Start quantizing block model.layers 23/32\n",
      "2024-09-07 19:37:59,395 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 20:00:37,228 - quantizer - INFO - Quantizing self_attn.q_proj in block 23/32...\n",
      "2024-09-07 20:01:23,587 - quantizer - INFO - Quantizing self_attn.k_proj in block 23/32...\n",
      "2024-09-07 20:01:37,884 - quantizer - INFO - Quantizing self_attn.v_proj in block 23/32...\n",
      "2024-09-07 20:01:58,136 - quantizer - INFO - Block 23 initial loss 20.242990493774414\n",
      "2024-09-07 20:02:12,799 - quantizer - INFO - Block 23 @ epoch 0 new loss 17.135108947753906 old loss 20.242990493774414 BETTER\n",
      "2024-09-07 20:02:27,034 - quantizer - INFO - Block 23 @ epoch 1 new loss 17.079713821411133 old loss 17.135108947753906 BETTER\n",
      "2024-09-07 20:02:40,378 - quantizer - INFO - Block 23 @ epoch 2 new loss 16.81470489501953 old loss 17.079713821411133 BETTER\n",
      "2024-09-07 20:02:54,337 - quantizer - INFO - Block 23 @ epoch 3 new loss 16.651378631591797 old loss 16.81470489501953 BETTER\n",
      "2024-09-07 20:03:08,999 - quantizer - INFO - Block 23 @ epoch 4 new loss 16.085317611694336 old loss 16.651378631591797 BETTER\n",
      "2024-09-07 20:03:09,508 - quantizer - INFO - Quantizing self_attn.o_proj in block 23/32...\n",
      "2024-09-07 20:04:00,889 - quantizer - INFO - Block 23 initial loss 125021.859375\n",
      "2024-09-07 20:04:41,894 - quantizer - INFO - Quantizing mlp.gate_proj in block 23/32...\n",
      "2024-09-07 20:07:17,319 - quantizer - INFO - Quantizing mlp.up_proj in block 23/32...\n",
      "2024-09-07 20:09:56,959 - quantizer - INFO - Block 23 initial loss 5509842.0\n",
      "2024-09-07 20:10:33,135 - quantizer - INFO - Quantizing mlp.down_proj in block 23/32...\n",
      "2024-09-07 20:13:17,087 - quantizer - INFO - Start quantizing block model.layers 24/32\n",
      "2024-09-07 20:13:17,088 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 20:35:55,280 - quantizer - INFO - Quantizing self_attn.q_proj in block 24/32...\n",
      "2024-09-07 20:36:42,193 - quantizer - INFO - Quantizing self_attn.k_proj in block 24/32...\n",
      "2024-09-07 20:36:56,489 - quantizer - INFO - Quantizing self_attn.v_proj in block 24/32...\n",
      "2024-09-07 20:37:16,525 - quantizer - INFO - Block 24 initial loss 15.182829856872559\n",
      "2024-09-07 20:37:32,438 - quantizer - INFO - Block 24 @ epoch 0 new loss 12.694854736328125 old loss 15.182829856872559 BETTER\n",
      "2024-09-07 20:37:59,193 - quantizer - INFO - Block 24 @ epoch 2 new loss 12.655941009521484 old loss 12.694854736328125 BETTER\n",
      "2024-09-07 20:38:12,996 - quantizer - INFO - Block 24 @ epoch 3 new loss 12.515689849853516 old loss 12.655941009521484 BETTER\n",
      "2024-09-07 20:38:26,561 - quantizer - INFO - Quantizing self_attn.o_proj in block 24/32...\n",
      "2024-09-07 20:39:18,478 - quantizer - INFO - Block 24 initial loss 95914.9296875\n",
      "2024-09-07 20:40:00,998 - quantizer - INFO - Quantizing mlp.gate_proj in block 24/32...\n",
      "2024-09-07 20:42:35,406 - quantizer - INFO - Quantizing mlp.up_proj in block 24/32...\n",
      "2024-09-07 20:45:13,197 - quantizer - INFO - Block 24 initial loss 9338792.0\n",
      "2024-09-07 20:45:52,008 - quantizer - INFO - Quantizing mlp.down_proj in block 24/32...\n",
      "2024-09-07 20:48:35,122 - quantizer - INFO - Start quantizing block model.layers 25/32\n",
      "2024-09-07 20:48:35,125 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725cddd80b014e12b04bc018674d89c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 21:11:14,675 - quantizer - INFO - Quantizing self_attn.q_proj in block 25/32...\n",
      "2024-09-07 21:12:01,113 - quantizer - INFO - Quantizing self_attn.k_proj in block 25/32...\n",
      "2024-09-07 21:12:15,316 - quantizer - INFO - Quantizing self_attn.v_proj in block 25/32...\n",
      "2024-09-07 21:12:35,108 - quantizer - INFO - Block 25 initial loss 19.050060272216797\n",
      "2024-09-07 21:12:50,203 - quantizer - INFO - Block 25 @ epoch 0 new loss 16.047752380371094 old loss 19.050060272216797 BETTER\n",
      "2024-09-07 21:13:04,302 - quantizer - INFO - Block 25 @ epoch 1 new loss 16.001928329467773 old loss 16.047752380371094 BETTER\n",
      "2024-09-07 21:13:18,368 - quantizer - INFO - Block 25 @ epoch 2 new loss 15.880873680114746 old loss 16.001928329467773 BETTER\n",
      "2024-09-07 21:13:31,061 - quantizer - INFO - Block 25 @ epoch 3 new loss 15.782022476196289 old loss 15.880873680114746 BETTER\n",
      "2024-09-07 21:13:46,451 - quantizer - INFO - Quantizing self_attn.o_proj in block 25/32...\n",
      "2024-09-07 21:14:38,970 - quantizer - INFO - Block 25 initial loss 110715.234375\n",
      "2024-09-07 21:14:55,040 - quantizer - INFO - Block 25 @ epoch 0 new loss 110061.828125 old loss 110715.234375 BETTER\n",
      "2024-09-07 21:15:22,538 - quantizer - INFO - Block 25 @ epoch 2 new loss 110036.9296875 old loss 110061.828125 BETTER\n",
      "2024-09-07 21:15:48,105 - quantizer - INFO - Quantizing mlp.gate_proj in block 25/32...\n",
      "2024-09-07 21:18:22,533 - quantizer - INFO - Quantizing mlp.up_proj in block 25/32...\n",
      "2024-09-07 21:21:00,721 - quantizer - INFO - Block 25 initial loss 11163990.0\n",
      "2024-09-07 21:21:35,565 - quantizer - INFO - Quantizing mlp.down_proj in block 25/32...\n",
      "2024-09-07 21:24:17,893 - quantizer - INFO - Start quantizing block model.layers 26/32\n",
      "2024-09-07 21:24:17,896 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ca0bcc5ed544e39eff158f0aee14be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 21:46:56,817 - quantizer - INFO - Quantizing self_attn.q_proj in block 26/32...\n",
      "2024-09-07 21:47:42,966 - quantizer - INFO - Quantizing self_attn.k_proj in block 26/32...\n",
      "2024-09-07 21:47:57,829 - quantizer - INFO - Quantizing self_attn.v_proj in block 26/32...\n",
      "2024-09-07 21:48:19,363 - quantizer - INFO - Block 26 initial loss 24.082275390625\n",
      "2024-09-07 21:48:35,920 - quantizer - INFO - Block 26 @ epoch 0 new loss 19.94071388244629 old loss 24.082275390625 BETTER\n",
      "2024-09-07 21:48:50,417 - quantizer - INFO - Block 26 @ epoch 1 new loss 19.848508834838867 old loss 19.94071388244629 BETTER\n",
      "2024-09-07 21:49:05,223 - quantizer - INFO - Block 26 @ epoch 2 new loss 19.778854370117188 old loss 19.848508834838867 BETTER\n",
      "2024-09-07 21:49:33,364 - quantizer - INFO - Block 26 @ epoch 4 new loss 19.773700714111328 old loss 19.778854370117188 BETTER\n",
      "2024-09-07 21:49:34,155 - quantizer - INFO - Quantizing self_attn.o_proj in block 26/32...\n",
      "2024-09-07 21:50:25,997 - quantizer - INFO - Block 26 initial loss 139689.796875\n",
      "2024-09-07 21:50:41,573 - quantizer - INFO - Block 26 @ epoch 0 new loss 139492.796875 old loss 139689.796875 BETTER\n",
      "2024-09-07 21:51:20,494 - quantizer - INFO - Quantizing mlp.gate_proj in block 26/32...\n",
      "2024-09-07 21:53:55,690 - quantizer - INFO - Quantizing mlp.up_proj in block 26/32...\n",
      "2024-09-07 21:56:34,762 - quantizer - INFO - Block 26 initial loss 3945310.75\n",
      "2024-09-07 21:57:09,699 - quantizer - INFO - Quantizing mlp.down_proj in block 26/32...\n",
      "2024-09-07 21:59:53,615 - quantizer - INFO - Start quantizing block model.layers 27/32\n",
      "2024-09-07 21:59:53,616 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c7086327e0453d85c6ddd7341a2071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 22:22:32,425 - quantizer - INFO - Quantizing self_attn.q_proj in block 27/32...\n",
      "2024-09-07 22:23:18,363 - quantizer - INFO - Quantizing self_attn.k_proj in block 27/32...\n",
      "2024-09-07 22:23:32,911 - quantizer - INFO - Quantizing self_attn.v_proj in block 27/32...\n",
      "2024-09-07 22:23:52,815 - quantizer - INFO - Block 27 initial loss 36.93788528442383\n",
      "2024-09-07 22:24:08,767 - quantizer - INFO - Block 27 @ epoch 0 new loss 32.252437591552734 old loss 36.93788528442383 BETTER\n",
      "2024-09-07 22:24:36,330 - quantizer - INFO - Block 27 @ epoch 2 new loss 32.07261657714844 old loss 32.252437591552734 BETTER\n",
      "2024-09-07 22:24:49,547 - quantizer - INFO - Block 27 @ epoch 3 new loss 31.862533569335938 old loss 32.07261657714844 BETTER\n",
      "2024-09-07 22:25:03,306 - quantizer - INFO - Block 27 @ epoch 4 new loss 31.801422119140625 old loss 31.862533569335938 BETTER\n",
      "2024-09-07 22:25:04,094 - quantizer - INFO - Quantizing self_attn.o_proj in block 27/32...\n",
      "2024-09-07 22:25:56,051 - quantizer - INFO - Block 27 initial loss 216891.296875\n",
      "2024-09-07 22:26:11,251 - quantizer - INFO - Block 27 @ epoch 0 new loss 213424.109375 old loss 216891.296875 BETTER\n",
      "2024-09-07 22:26:24,080 - quantizer - INFO - Block 27 @ epoch 1 new loss 213197.90625 old loss 213424.109375 BETTER\n",
      "2024-09-07 22:26:37,132 - quantizer - INFO - Block 27 @ epoch 2 new loss 212698.015625 old loss 213197.90625 BETTER\n",
      "2024-09-07 22:27:04,009 - quantizer - INFO - Quantizing mlp.gate_proj in block 27/32...\n",
      "2024-09-07 22:29:39,501 - quantizer - INFO - Quantizing mlp.up_proj in block 27/32...\n",
      "2024-09-07 22:32:19,326 - quantizer - INFO - Block 27 initial loss 1737307.375\n",
      "2024-09-07 22:32:57,565 - quantizer - INFO - Quantizing mlp.down_proj in block 27/32...\n",
      "2024-09-07 22:35:40,381 - quantizer - INFO - Start quantizing block model.layers 28/32\n",
      "2024-09-07 22:35:40,382 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dafdeedb374ff0b5567aeccaf3209b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 22:58:20,062 - quantizer - INFO - Quantizing self_attn.q_proj in block 28/32...\n",
      "2024-09-07 22:59:06,476 - quantizer - INFO - Quantizing self_attn.k_proj in block 28/32...\n",
      "2024-09-07 22:59:21,132 - quantizer - INFO - Quantizing self_attn.v_proj in block 28/32...\n",
      "2024-09-07 22:59:41,597 - quantizer - INFO - Block 28 initial loss 44.70111846923828\n",
      "2024-09-07 22:59:57,512 - quantizer - INFO - Block 28 @ epoch 0 new loss 40.089988708496094 old loss 44.70111846923828 BETTER\n",
      "2024-09-07 23:00:11,951 - quantizer - INFO - Block 28 @ epoch 1 new loss 40.019874572753906 old loss 40.089988708496094 BETTER\n",
      "2024-09-07 23:00:25,975 - quantizer - INFO - Block 28 @ epoch 2 new loss 39.82428741455078 old loss 40.019874572753906 BETTER\n",
      "2024-09-07 23:00:40,006 - quantizer - INFO - Block 28 @ epoch 3 new loss 39.53759002685547 old loss 39.82428741455078 BETTER\n",
      "2024-09-07 23:00:54,726 - quantizer - INFO - Block 28 @ epoch 4 new loss 39.21992874145508 old loss 39.53759002685547 BETTER\n",
      "2024-09-07 23:00:55,612 - quantizer - INFO - Quantizing self_attn.o_proj in block 28/32...\n",
      "2024-09-07 23:01:47,166 - quantizer - INFO - Block 28 initial loss 255410.40625\n",
      "2024-09-07 23:02:02,109 - quantizer - INFO - Block 28 @ epoch 0 new loss 255052.953125 old loss 255410.40625 BETTER\n",
      "2024-09-07 23:02:15,876 - quantizer - INFO - Block 28 @ epoch 1 new loss 254685.953125 old loss 255052.953125 BETTER\n",
      "2024-09-07 23:02:57,621 - quantizer - INFO - Quantizing mlp.gate_proj in block 28/32...\n",
      "2024-09-07 23:05:33,146 - quantizer - INFO - Quantizing mlp.up_proj in block 28/32...\n",
      "2024-09-07 23:08:12,460 - quantizer - INFO - Block 28 initial loss 9623454.0\n",
      "2024-09-07 23:08:47,939 - quantizer - INFO - Quantizing mlp.down_proj in block 28/32...\n",
      "2024-09-07 23:11:31,281 - quantizer - INFO - Start quantizing block model.layers 29/32\n",
      "2024-09-07 23:11:31,283 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d739be68c36411d872673583ef304b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 23:34:10,306 - quantizer - INFO - Quantizing self_attn.q_proj in block 29/32...\n",
      "2024-09-07 23:34:56,389 - quantizer - INFO - Quantizing self_attn.k_proj in block 29/32...\n",
      "2024-09-07 23:35:11,037 - quantizer - INFO - Quantizing self_attn.v_proj in block 29/32...\n",
      "2024-09-07 23:35:31,874 - quantizer - INFO - Block 29 initial loss 52.27888488769531\n",
      "2024-09-07 23:35:48,252 - quantizer - INFO - Block 29 @ epoch 0 new loss 48.66423797607422 old loss 52.27888488769531 BETTER\n",
      "2024-09-07 23:36:28,380 - quantizer - INFO - Block 29 @ epoch 3 new loss 48.58110046386719 old loss 48.66423797607422 BETTER\n",
      "2024-09-07 23:36:42,673 - quantizer - INFO - Block 29 @ epoch 4 new loss 48.38151931762695 old loss 48.58110046386719 BETTER\n",
      "2024-09-07 23:36:43,724 - quantizer - INFO - Quantizing self_attn.o_proj in block 29/32...\n",
      "2024-09-07 23:37:35,058 - quantizer - INFO - Block 29 initial loss 290860.1875\n",
      "2024-09-07 23:37:50,432 - quantizer - INFO - Block 29 @ epoch 0 new loss 283942.0625 old loss 290860.1875 BETTER\n",
      "2024-09-07 23:38:32,061 - quantizer - INFO - Quantizing mlp.gate_proj in block 29/32...\n",
      "2024-09-07 23:41:06,151 - quantizer - INFO - Quantizing mlp.up_proj in block 29/32...\n",
      "2024-09-07 23:43:45,479 - quantizer - INFO - Block 29 initial loss 28035352.0\n",
      "2024-09-07 23:44:22,918 - quantizer - INFO - Quantizing mlp.down_proj in block 29/32...\n",
      "2024-09-07 23:47:07,172 - quantizer - INFO - Start quantizing block model.layers 30/32\n",
      "2024-09-07 23:47:07,175 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54f68b14e2a4e97853c4ddeacbd4450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 00:09:44,766 - quantizer - INFO - Quantizing self_attn.q_proj in block 30/32...\n",
      "2024-09-08 00:10:31,117 - quantizer - INFO - Quantizing self_attn.k_proj in block 30/32...\n",
      "2024-09-08 00:10:45,468 - quantizer - INFO - Quantizing self_attn.v_proj in block 30/32...\n",
      "2024-09-08 00:11:06,348 - quantizer - INFO - Block 30 initial loss 95.32785034179688\n",
      "2024-09-08 00:11:21,320 - quantizer - INFO - Block 30 @ epoch 0 new loss 87.77876281738281 old loss 95.32785034179688 BETTER\n",
      "2024-09-08 00:11:35,343 - quantizer - INFO - Block 30 @ epoch 1 new loss 87.28673553466797 old loss 87.77876281738281 BETTER\n",
      "2024-09-08 00:11:49,859 - quantizer - INFO - Block 30 @ epoch 2 new loss 87.14395904541016 old loss 87.28673553466797 BETTER\n",
      "2024-09-08 00:12:03,068 - quantizer - INFO - Block 30 @ epoch 3 new loss 86.92432403564453 old loss 87.14395904541016 BETTER\n",
      "2024-09-08 00:12:17,077 - quantizer - INFO - Block 30 @ epoch 4 new loss 86.55249786376953 old loss 86.92432403564453 BETTER\n",
      "2024-09-08 00:12:17,694 - quantizer - INFO - Quantizing self_attn.o_proj in block 30/32...\n",
      "2024-09-08 00:13:10,106 - quantizer - INFO - Block 30 initial loss 515408.40625\n",
      "2024-09-08 00:13:25,373 - quantizer - INFO - Block 30 @ epoch 0 new loss 510730.5 old loss 515408.40625 BETTER\n",
      "2024-09-08 00:14:05,678 - quantizer - INFO - Quantizing mlp.gate_proj in block 30/32...\n",
      "2024-09-08 00:16:40,124 - quantizer - INFO - Quantizing mlp.up_proj in block 30/32...\n",
      "2024-09-08 00:19:19,786 - quantizer - INFO - Block 30 initial loss 15832093.0\n",
      "2024-09-08 00:19:56,406 - quantizer - INFO - Quantizing mlp.down_proj in block 30/32...\n",
      "2024-09-08 00:22:39,839 - quantizer - INFO - Start quantizing block model.layers 31/32\n",
      "2024-09-08 00:22:39,841 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb54bfc404a43e0ab06be5fa61a97ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 00:45:18,206 - quantizer - INFO - Quantizing self_attn.q_proj in block 31/32...\n",
      "2024-09-08 00:46:04,653 - quantizer - INFO - Quantizing self_attn.k_proj in block 31/32...\n",
      "2024-09-08 00:46:19,452 - quantizer - INFO - Quantizing self_attn.v_proj in block 31/32...\n",
      "2024-09-08 00:46:39,424 - quantizer - INFO - Block 31 initial loss 110.40959930419922\n",
      "2024-09-08 00:46:56,003 - quantizer - INFO - Block 31 @ epoch 0 new loss 101.96656036376953 old loss 110.40959930419922 BETTER\n",
      "2024-09-08 00:47:10,354 - quantizer - INFO - Block 31 @ epoch 1 new loss 101.14036560058594 old loss 101.96656036376953 BETTER\n",
      "2024-09-08 00:47:24,276 - quantizer - INFO - Block 31 @ epoch 2 new loss 100.34866333007812 old loss 101.14036560058594 BETTER\n",
      "2024-09-08 00:47:38,719 - quantizer - INFO - Block 31 @ epoch 3 new loss 98.1333999633789 old loss 100.34866333007812 BETTER\n",
      "2024-09-08 00:47:52,661 - quantizer - INFO - Block 31 @ epoch 4 new loss 97.24917602539062 old loss 98.1333999633789 BETTER\n",
      "2024-09-08 00:47:53,758 - quantizer - INFO - Quantizing self_attn.o_proj in block 31/32...\n",
      "2024-09-08 00:48:45,192 - quantizer - INFO - Block 31 initial loss 511703.25\n",
      "2024-09-08 00:49:00,316 - quantizer - INFO - Block 31 @ epoch 0 new loss 510131.15625 old loss 511703.25 BETTER\n",
      "2024-09-08 00:49:40,272 - quantizer - INFO - Block 31 @ epoch 3 new loss 509705.34375 old loss 510131.15625 BETTER\n",
      "2024-09-08 00:49:54,410 - quantizer - INFO - Block 31 @ epoch 4 new loss 509308.71875 old loss 509705.34375 BETTER\n",
      "2024-09-08 00:49:55,380 - quantizer - INFO - Quantizing mlp.gate_proj in block 31/32...\n",
      "2024-09-08 00:52:28,862 - quantizer - INFO - Quantizing mlp.up_proj in block 31/32...\n",
      "2024-09-08 00:55:07,269 - quantizer - INFO - Block 31 initial loss 10973392.0\n",
      "2024-09-08 00:55:43,132 - quantizer - INFO - Quantizing mlp.down_proj in block 31/32...\n",
      "2024-09-08 00:58:25,857 - quantizer - INFO - Start quantizing block model.layers 32/32\n",
      "2024-09-08 00:58:25,860 - quantizer - INFO - Module to quantize ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46abc8deb1ef432482c4b492517e822f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 01:21:05,006 - quantizer - INFO - Quantizing self_attn.q_proj in block 32/32...\n",
      "2024-09-08 01:21:51,718 - quantizer - INFO - Quantizing self_attn.k_proj in block 32/32...\n",
      "2024-09-08 01:22:05,950 - quantizer - INFO - Quantizing self_attn.v_proj in block 32/32...\n",
      "2024-09-08 01:22:26,216 - quantizer - INFO - Block 32 initial loss 357.33258056640625\n",
      "2024-09-08 01:22:41,990 - quantizer - INFO - Block 32 @ epoch 0 new loss 353.2872314453125 old loss 357.33258056640625 BETTER\n",
      "2024-09-08 01:23:21,745 - quantizer - INFO - Block 32 @ epoch 3 new loss 351.0727844238281 old loss 353.2872314453125 BETTER\n",
      "2024-09-08 01:23:35,326 - quantizer - INFO - Quantizing self_attn.o_proj in block 32/32...\n",
      "2024-09-08 01:24:27,379 - quantizer - INFO - Block 32 initial loss 1647864.25\n",
      "2024-09-08 01:25:08,925 - quantizer - INFO - Quantizing mlp.gate_proj in block 32/32...\n",
      "2024-09-08 01:27:43,235 - quantizer - INFO - Quantizing mlp.up_proj in block 32/32...\n",
      "2024-09-08 01:30:22,550 - quantizer - INFO - Block 32 initial loss 821437824.0\n",
      "2024-09-08 01:30:58,631 - quantizer - INFO - Quantizing mlp.down_proj in block 32/32...\n",
      "2024-09-08 01:37:13,303 - quantizer - INFO - End2end initial loss nan\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-09-08 02:02:31,967 - accelerate.accelerator - INFO - Model weights saved in /root/llm_quant_safety/quantization/models/Llama-3.1-8B-Instruct-quip-2bit/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "          (k_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "          (q_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (act_fn): SiLU()\n",
       "          (gate_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant.quantize_model(model, tokenizer, quant_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e866d1d-a6d6-4268-a154-a40cefba153f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "          (k_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "          (q_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (act_fn): SiLU()\n",
       "          (gate_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            (codebook): E8P12_codebook()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d6808-1822-414b-9507-8053f6e90948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef5e2d6-06fd-4968-ac45-f53b116c4c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d4103-4cbb-4b11-95b5-b4fd05b4dc35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7ddf8dc-a662-4498-ad16-1f54c524729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 06:04:55,250 - accelerate.accelerator - INFO - Model weights saved in /root/llm_quant_safety/quantization/models/Mistral-7B-Instruct-v0.2-quip-2bit/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "quant.save(model, quant_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28139c60-bf83-483f-8d19-b3d84f555915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd5a1a-c0d9-4425-8b1a-fdadd04b72b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed07666-6634-4e04-8e11-d69aa0e0cc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab5878-791b-47dc-aac8-9ac7488bd12e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c0891-f03b-4fdf-989b-7ebea45fbc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc8f2e-ee24-484f-b40a-fac697fe33ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a12f98-e1e7-47b8-aeb7-73497f301a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95fd210a-246f-4fb8-8b92-512c647ef848",
   "metadata": {},
   "source": [
    "### try inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22d74939-9ef3-4909-879b-b19bf7311b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.padding_side=\"left\"\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc653f8-7334-4063-9254-b43a8d5fe618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_ids = tokenizer.encode(\"The capital of France is\", return_tensors=\"pt\").cuda()\n",
    "# print(tokenizer.decode(model.generate(input_ids, do_sample=True, temperature=1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b2cc1-d351-45c8-8cb5-d62e028a0c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365fd395-7658-435e-8eab-db78dca6fde3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904555ea-3030-420b-a1cf-9609703d31c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7305dd9a-bf3b-4e3a-b99d-cad360645c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fa72ac3-d7b3-4714-8d09-0c105d5851dd",
   "metadata": {},
   "source": [
    "# TMP quantizer exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16e398a2-9c67-4973-b817-a713c2145b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_block_name_with_pattern, get_preceding_modules, recurse_getattr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e28571f8-5945-4217-945f-94a2e4e37ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.layers'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_block_name_with_pattern(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4abe21ce-d0f8-4034-845c-f8037c83c675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-31): 32 x LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm()\n",
       "    (post_attention_layernorm): LlamaRMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b42d3b9b-c257-4606-9824-0f6cab56f857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.embed_tokens']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_preceding_modules(model, 'model.layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a98f37b-7111-4484-8718-e696a5ed8479",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = recurse_getattr(model, 'model.layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16198266-1220-457f-8eae-8272d4e95051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-31): 32 x LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm()\n",
       "    (post_attention_layernorm): LlamaRMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5445a1d9-1cef-4554-87bc-57f7271cf58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_names_after_last_block = get_preceding_modules(model, 'model.layers', reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e10ccac4-1e13-4bd6-a90f-7d3eb894adef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lm_head', 'model.rotary_emb', 'model.norm']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_names_after_last_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d363c-b4c8-454c-ac44-ac6f0c0396c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quip",
   "language": "python",
   "name": "quip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
