{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba03670-977e-4f04-8c95-e64ba19c47ee",
   "metadata": {},
   "source": [
    "# vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3930107a-eb29-4697-9df0-2bd06ebf0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc64e689-3cac-43c1-befd-1e8bb1e66e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90010fab-1786-44c7-ad8c-0eaa4f19854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cc81160-67b9-438c-812f-d38b7605707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9eaf8-caef-4209-9539-13e8ee9e8793",
   "metadata": {},
   "source": [
    "# data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0d5b99-f6b6-4e66-a354-6c377eb826d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9522cca-7239-469e-8651-ed7b57868e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55998 entries, 0 to 55997\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   prompt         55998 non-null  object\n",
      " 1   lang           55998 non-null  object\n",
      " 2   scenario       55998 non-null  object\n",
      " 3   eng_prompt     55998 non-null  object\n",
      " 4   method         55998 non-null  object\n",
      " 5   model          55998 non-null  object\n",
      " 6   tokenized_len  55998 non-null  int64 \n",
      " 7   source_idx     55998 non-null  int64 \n",
      " 8   response       55998 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('data/llama_31_8b_answers.csv')\n",
    "# df = pd.read_csv('data/llama_quip4.csv')\n",
    "df = pd.read_csv('data/responses_quip2.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dfb80f-446b-42b7-aea2-ff62db3df09d",
   "metadata": {},
   "source": [
    "# no quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e541cb-2d27-4832-97a2-0b6b91975be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-08 11:36:53 config.py:813] Defaulting to use mp for distributed inference\n",
      "WARNING 09-08 11:36:53 arg_utils.py:839] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 09-08 11:36:53 config.py:911] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 09-08 11:36:53 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated', speculative_config=None, tokenizer='mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 09-08 11:36:54 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-08 11:36:54 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=729361)\u001b[0;0m INFO 09-08 11:36:54 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-08 11:36:56 utils.py:975] Found nccl from library libnccl.so.2\n",
      "INFO 09-08 11:36:56 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=729361)\u001b[0;0m INFO 09-08 11:36:56 utils.py:975] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=729361)\u001b[0;0m INFO 09-08 11:36:56 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-08 11:36:57 custom_all_reduce_utils.py:203] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "INFO 09-08 11:37:17 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=729361)\u001b[0;0m INFO 09-08 11:37:17 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "INFO 09-08 11:37:17 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fdff6fb19a0>, local_subscribe_port=43935, remote_subscribe_port=None)\n",
      "INFO 09-08 11:37:17 model_runner.py:879] Starting to load model mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=729361)\u001b[0;0m INFO 09-08 11:37:17 model_runner.py:879] Starting to load model mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated...\n",
      "INFO 09-08 11:37:18 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=729361)\u001b[0;0m INFO 09-08 11:37:19 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059be3a97cfa4319a136f56641c2f0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=729361)\u001b[0;0m INFO 09-08 11:37:27 model_runner.py:890] Loading model weights took 7.5122 GB\n",
      "INFO 09-08 11:37:27 model_runner.py:890] Loading model weights took 7.5122 GB\n",
      "INFO 09-08 11:37:29 distributed_gpu_executor.py:56] # GPU blocks: 31624, # CPU blocks: 4096\n",
      "INFO 09-08 11:37:35 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=729361)\u001b[0;0m INFO 09-08 11:37:35 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-08 11:37:35 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=729361)\u001b[0;0m INFO 09-08 11:37:35 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-08 11:38:09 custom_all_reduce.py:223] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=729361)\u001b[0;0m INFO 09-08 11:38:09 custom_all_reduce.py:223] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=729361)\u001b[0;0m INFO 09-08 11:38:09 model_runner.py:1300] Graph capturing finished in 34 secs.\n",
      "INFO 09-08 11:38:09 model_runner.py:1300] Graph capturing finished in 34 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=model_name, \n",
    "    dtype=torch.bfloat16, \n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a254bfe8-6dfa-4602-9963-d19b140c85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_prompt = lambda text: f\"\"\\\n",
    "    f\"You are school English teacher\\n\"\\\n",
    "    f\"Translate following text into English '{text}'\\n\"\\\n",
    "    f\"Provide only translation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0a4df8a-4545-4499-8ee6-01adcf9facd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_translation_prompt(prompt):\n",
    "    user_message = translation_prompt(prompt)\n",
    "    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b1591d0-f8c9-4347-861f-31a6c31c780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_prompts = df.apply(\n",
    "    lambda row: generate_translation_prompt(row.response), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e04c434b-0f84-4a66-9f61-ea19c7d9e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.6, \n",
    "    top_p=0.9, \n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97e4d479-b80c-4d29-8c94-05dc6ed66bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆ| 55998/55998 [1:04:35<00:00, 14.45it/s, est. speed input: 2892.51 toks/s, ou\n"
     ]
    }
   ],
   "source": [
    "outputs_translate = llm.generate(\n",
    "    list(translate_prompts), \n",
    "    sampling_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b996cf78-1a63-4a43-b361-cf2a95da918b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translated = [out.outputs[0].text for out in outputs_translate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0755f678-765e-497c-8711-3973373903ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_fixed = []\n",
    "for tr in translated:\n",
    "    if 'here is the translation:' in tr.lower():\n",
    "        translation_fixed.append(tr[24:].strip())\n",
    "    else:\n",
    "        translation_fixed.append(tr.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f13d036e-ee28-4851-a4cf-1348f873a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['translated_response'] = translation_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1354c078-a291-42c5-854f-63cfafa9b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/translated_quip2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe767a6-38cc-4b3c-95cd-7272292d9123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001106d-524f-4e6c-a0e2-bb64c86df7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29055e1-d84d-4f38-b72e-dda9d93a77aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecf8cf-fdd5-4bb2-b7b4-5c6fc386f402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef4b8e-b72f-47a0-ad5a-4ffbdbad5834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
